{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "058e0175-ef36-46b5-b2b1-1d35d258311d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import tensorflow as tf\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "SEQ_LEN = 6                 \n",
    "HORIZONS = 3               \n",
    "PATCH_SIZE = 13             \n",
    "HALF = PATCH_SIZE // 2\n",
    "FILL_NAN_VALUE = 0.0\n",
    "\n",
    "REQUIRED_COLS = [\n",
    "    \"era5_t2m_file\", \"era5_d2m_file\", \"era5_tp_file\",\n",
    "    \"era5_u10_file\", \"era5_v10_file\",\n",
    "    \"viirs_file\", \"dem_file\", \"lulc_file\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f5774f7-5a3f-4c9b-b70b-bfdfafab3ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.19.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ae142a5-292c-4193-9e60-d501771b46db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_single_raster(path):\n",
    "    with rasterio.open(path) as src:\n",
    "        arr = src.read()  # shape = (bands, H, W)\n",
    "\n",
    "    if arr.shape[0] == 1:\n",
    "        # single-band → squeeze to (H, W)\n",
    "        return arr[0]\n",
    "    else:\n",
    "        # multi-band (like VIIRS) → keep (bands, H, W)\n",
    "        return arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "38d95c89-2d72-4f83-897d-966405060ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rasters(df, raster_cols, max_workers=8):\n",
    "    \"\"\"Load unique rasters into memory, cached, with multithreading.\"\"\"\n",
    "    all_paths = set()\n",
    "\n",
    "    for col in raster_cols:\n",
    "        if col in df.columns:\n",
    "            all_paths.update(df[col].dropna().unique())\n",
    "    all_paths = list(all_paths)\n",
    "\n",
    "    cache = {}\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        results = list(ex.map(_load_single_raster, all_paths))\n",
    "\n",
    "    for path, arr in zip(all_paths, results):\n",
    "        if arr is not None:\n",
    "            cache[path] = arr   # ⚠️ for VIIRS this will be shape (bands, H, W)\n",
    "    return cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "edee5feb-0f88-4090-ab3f-4c28838c8f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _safe_center(h, w, patch_size=PATCH_SIZE):\n",
    "    half = patch_size // 2\n",
    "    r = np.clip(h // 2, half, h - half - 1)\n",
    "    c = np.clip(w // 2, half, w - half - 1)\n",
    "    return r, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4fce6a62-85f9-4b91-a739-a40022b4718d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_patch(arr, row, col, patch_size=PATCH_SIZE):\n",
    "    half = patch_size // 2\n",
    "    h, w = arr.shape\n",
    "\n",
    "    r0 = row - half\n",
    "    r1 = row + half + 1\n",
    "    c0 = col - half\n",
    "    c1 = col + half + 1\n",
    "\n",
    "    # initialize empty patch with zeros\n",
    "    patch = np.zeros((patch_size, patch_size), dtype=arr.dtype)\n",
    "\n",
    "    # compute overlap between patch and array\n",
    "    r0_clip = max(r0, 0)\n",
    "    r1_clip = min(r1, h)\n",
    "    c0_clip = max(c0, 0)\n",
    "    c1_clip = min(c1, w)\n",
    "\n",
    "    # where to paste inside patch\n",
    "    pr0 = r0_clip - r0\n",
    "    pr1 = pr0 + (r1_clip - r0_clip)\n",
    "    pc0 = c0_clip - c0\n",
    "    pc1 = pc0 + (c1_clip - c0_clip)\n",
    "\n",
    "    # copy valid region\n",
    "    patch[pr0:pr1, pc0:pc1] = arr[r0_clip:r1_clip, c0_clip:c1_clip]\n",
    "\n",
    "    return patch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e402e30a-b656-4fe1-9eb9-99fc3e3171a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sample(seq_rows, horizon_rows, cache, force_fire=False):\n",
    "    seq_patches = []\n",
    "\n",
    "    # ---------- X sequence ----------\n",
    "    for _, row in seq_rows.iterrows():\n",
    "        bands = []\n",
    "        for var in [\"era5_t2m_file\", \"era5_d2m_file\", \"era5_tp_file\",\n",
    "                    \"era5_u10_file\", \"era5_v10_file\"]:\n",
    "            arr = cache[row[var]]\n",
    "\n",
    "            if len(arr.shape) == 3:\n",
    "                arr = arr[0]\n",
    "\n",
    "            h, w = arr.shape\n",
    "            r, c = _safe_center(h, w)\n",
    "            bands.append(_extract_patch(arr, r, c))\n",
    "\n",
    "        # static vars\n",
    "        dem = cache[row[\"dem_file\"]]\n",
    "        lulc = cache[row[\"lulc_file\"]]\n",
    "\n",
    "        if len(dem.shape) == 3:\n",
    "            dem = dem[0]\n",
    "        if len(lulc.shape) == 3:\n",
    "            lulc = lulc[0]\n",
    "\n",
    "        h, w = dem.shape\n",
    "        r, c = _safe_center(h, w)\n",
    "        bands.append(_extract_patch(dem, r, c))\n",
    "        bands.append(_extract_patch(lulc, r, c))\n",
    "\n",
    "        seq_patches.append(np.stack(bands, axis=-1))\n",
    "\n",
    "    X = np.stack(seq_patches, axis=0)\n",
    "\n",
    "    # ---------- Y horizon ----------\n",
    "    horizon_patches = []\n",
    "    # FIX: Loop over the horizon rows directly to extract one patch per row.\n",
    "    for _, row in horizon_rows.iterrows():\n",
    "        viirs_stack = cache[row[\"viirs_file\"]]\n",
    "        \n",
    "        # Get the target band index from the row.\n",
    "        # This assumes there's only one relevant band per horizon row.\n",
    "        target_band_idx_list = eval(row[\"target_band_idxs\"])\n",
    "        \n",
    "        # Take the first band index to match the HORIZONS shape.\n",
    "        idx = target_band_idx_list[0]\n",
    "        \n",
    "        band = viirs_stack[idx - 1]\n",
    "        h, w = band.shape\n",
    "        r, c = _safe_center(h, w)\n",
    "\n",
    "        if force_fire and np.any(band > 0):\n",
    "            fire_pos = np.argwhere(band > 0)\n",
    "            r, c = fire_pos[np.random.randint(len(fire_pos))]\n",
    "\n",
    "        horizon_patches.append(_extract_patch(band, r, c))\n",
    "\n",
    "    y = np.stack(horizon_patches, axis=0)\n",
    "\n",
    "    return X.astype(\"float32\"), y.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed58a017-a51e-4db8-aa39-6fcd8b5befb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "afdf22cf-fdf9-4d61-920d-8afba9b072c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator(df, cache, ensure_fire=True, fire_ratio=0.5):\n",
    "    \"\"\"\n",
    "    Creates a generator from a pre-split dataframe.\n",
    "    \"\"\"\n",
    "    # 1. First, identify all valid sequence start indices\n",
    "    valid_start_indices = [\n",
    "        i for i in range(len(df) - SEQ_LEN - HORIZONS + 1)\n",
    "    ]\n",
    "\n",
    "    # 2. Filter valid indices into 'fire' and 'non-fire' groups\n",
    "    fire_start_indices = []\n",
    "    non_fire_start_indices = []\n",
    "    \n",
    "    for i in valid_start_indices:\n",
    "        horizon_rows = df.iloc[i + SEQ_LEN : i + SEQ_LEN + HORIZONS]\n",
    "        has_fire = any(np.any(cache[row[\"viirs_file\"]] > 0) for _, row in horizon_rows.iterrows())\n",
    "        \n",
    "        if has_fire:\n",
    "            fire_start_indices.append(i)\n",
    "        else:\n",
    "            non_fire_start_indices.append(i)\n",
    "\n",
    "    # 3. Sample from the two groups based on fire_ratio\n",
    "    indices_to_use = []\n",
    "    \n",
    "    if ensure_fire and fire_start_indices:\n",
    "        # Number of samples to draw from each group\n",
    "        num_fire_samples = int(len(valid_start_indices) * fire_ratio)\n",
    "        num_non_fire_samples = len(valid_start_indices) - num_fire_samples\n",
    "\n",
    "        fire_indices = np.random.choice(\n",
    "            fire_start_indices, \n",
    "            size=min(num_fire_samples, len(fire_start_indices)), \n",
    "            replace=True\n",
    "        )\n",
    "        indices_to_use.extend(fire_indices)\n",
    "        \n",
    "        non_fire_indices = np.random.choice(\n",
    "            non_fire_start_indices, \n",
    "            size=min(num_non_fire_samples, len(non_fire_start_indices)), \n",
    "            replace=True\n",
    "        )\n",
    "        indices_to_use.extend(non_fire_indices)\n",
    "        \n",
    "        np.random.shuffle(indices_to_use)\n",
    "    else:\n",
    "        indices_to_use = valid_start_indices\n",
    "\n",
    "    # 4. Yield the samples\n",
    "    for i in indices_to_use:\n",
    "        seq_rows = df.iloc[i : i + SEQ_LEN]\n",
    "        horizon_rows = df.iloc[i + SEQ_LEN : i + SEQ_LEN + HORIZONS]\n",
    "        \n",
    "        X, y = build_sample(seq_rows, horizon_rows, cache, force_fire=False)\n",
    "        \n",
    "        yield X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dbccc812-c0bf-4446-9bbe-8e352a0c2ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(df, cache, shuffle=True, shuffle_buf=256,\n",
    "                     ensure_fire=True, fire_ratio=0.5):\n",
    "    output_signature = (\n",
    "        tf.TensorSpec(shape=(SEQ_LEN, PATCH_SIZE, PATCH_SIZE, 7), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(HORIZONS, PATCH_SIZE, PATCH_SIZE), dtype=tf.float32),\n",
    "    )\n",
    "    \n",
    "    ds = tf.data.Dataset.from_generator(\n",
    "        lambda: make_generator_from_df(df, cache, ensure_fire=ensure_fire, fire_ratio=fire_ratio),\n",
    "        output_signature=output_signature\n",
    "    )\n",
    "    \n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(shuffle_buf, reshuffle_each_iteration=True)\n",
    "\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e327f91e-e454-4b6c-8ea4-f6b8b6aae545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 17535\n",
      "Train samples: 14028\n",
      "Validation samples: 3507\n",
      "Loading rasters into memory...\n",
      "Loaded 9 rasters into memory ✅\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    csv_path = r\"C:\\Users\\Ankit\\Datasets_Forest_fire\\sequence_index_hourly_binary.csv\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # --- CORRECTED CODE STARTS HERE ---\n",
    "    # 1. Shuffle the entire DataFrame first to ensure a random split.\n",
    "    # We use a fixed random_state for reproducibility.\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # 2. Define the split ratio and calculate sizes.\n",
    "    TOTAL = len(df)\n",
    "    VAL_SPLIT = 0.2\n",
    "    val_size = int(TOTAL * VAL_SPLIT)\n",
    "\n",
    "    # 3. Split the DataFrame into training and validation subsets.\n",
    "    val_df = df.iloc[:val_size].copy()\n",
    "    train_df = df.iloc[val_size:].copy()\n",
    "\n",
    "    print(f\"Total samples: {TOTAL}\")\n",
    "    print(f\"Train samples: {len(train_df)}\")\n",
    "    print(f\"Validation samples: {len(val_df)}\")\n",
    "    # --- CORRECTED CODE ENDS HERE ---\n",
    "\n",
    "    raster_cols = REQUIRED_COLS\n",
    "    print(\"Loading rasters into memory...\")\n",
    "    # Load rasters from the *full* dataframe to ensure all are cached\n",
    "    cache = load_rasters(df, raster_cols, max_workers=8)\n",
    "    print(f\"Loaded {len(cache)} rasters into memory ✅\")\n",
    "\n",
    "    # 4. Create separate, balanced datasets from the split DataFrames.\n",
    "    # Note that create_dataset and make_generator will need to be\n",
    "    # updated to accept a DataFrame instead of a path.\n",
    "    train_dataset = create_dataset(train_df, cache, ensure_fire=True, fire_ratio=0.5)\n",
    "    val_dataset = create_dataset(val_df, cache, ensure_fire=True, fire_ratio=0.5)\n",
    "\n",
    "    # 5. Batch and prefetch the new datasets.\n",
    "    BATCH_SIZE = 2\n",
    "    train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "    val_dataset = val_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    # The rest of your model training code will go here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "103af332-026d-4a6f-9d6f-a11f17ef551b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL = 1000  \n",
    "VAL_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fce5a3e5-f7df-482e-851e-64a983e631e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.shuffle(1000, reshuffle_each_iteration=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4b1f3d67-7ef7-465c-a2a8-321ff13bfb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = int(TOTAL * VAL_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0be4f002-378a-4856-9040-4fe86aa5b5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = ds.take(val_size)\n",
    "train_dataset = ds.skip(val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3711b3f8-bdc6-41a5-9bab-91e406ada3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=2\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset   = val_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cf1c5c5c-a039-4939-b316-c9b3e4db71f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels: [0. 1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "all_labels = []\n",
    "for _, y in train_dataset.take(10):  # just check 10 batches\n",
    "    all_labels.extend(y.numpy().flatten())\n",
    "\n",
    "print(\"Unique labels:\", np.unique(all_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "02234fe6-6674-4d0d-9efb-a370d788a3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels found: [0. 1.]\n",
      "Label 0.0: 4319952 samples\n",
      "Label 1.0: 21489 samples\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "all_labels = []\n",
    "\n",
    "# Go through the entire dataset\n",
    "for _, y in train_dataset:  \n",
    "    all_labels.extend(y.numpy().flatten())\n",
    "\n",
    "unique_labels, counts = np.unique(all_labels, return_counts=True)\n",
    "\n",
    "print(\"Unique labels found:\", unique_labels)\n",
    "for lbl, cnt in zip(unique_labels, counts):\n",
    "    print(f\"Label {lbl}: {cnt} samples\")\n",
    "\n",
    "# Explicit checks\n",
    "if 0.0 not in unique_labels:\n",
    "    print(\"⚠️ 0 is absent in labels!\")\n",
    "if 1.0 not in unique_labels:\n",
    "    print(\"⚠️ 1 is absent in labels!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbd6271-29b2-4951-b20f-413cbaf2ccb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5fc03f-cd48-4ed7-bbca-f30245bc25ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "90893721-b56d-4630-b86d-cf7a92ef7fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e700a567-9a37-440c-9e04-d691f0824fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 6           \n",
    "PATCH_H = 13            \n",
    "PATCH_W = 13          \n",
    "CHANNELS = 7       \n",
    "HORIZONS = 3            \n",
    "LSTM_UNITS = 16      \n",
    "CNN_FEATURES = 64 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "27354090-4f0c-46e7-a8d1-0d0897ea2f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_lstm_model(seq_len=SEQ_LEN, patch_h=PATCH_H, patch_w=PATCH_W, channels=CHANNELS, horizons=HORIZONS):\n",
    "    inp = layers.Input(shape=(seq_len, patch_h, patch_w, channels))\n",
    "    \n",
    "    def build_cnn_block():\n",
    "        model = models.Sequential([\n",
    "            layers.Conv2D(32, (3,3), activation='relu', padding='same'),\n",
    "            layers.MaxPooling2D((2,2)),\n",
    "            layers.Conv2D(64, (3,3), activation='relu', padding='same'),\n",
    "            layers.MaxPooling2D((2,2)),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(CNN_FEATURES, activation='relu')\n",
    "        ])\n",
    "        return model\n",
    "\n",
    "    cnn = build_cnn_block()\n",
    "    td = layers.TimeDistributed(cnn)(inp)\n",
    "    \n",
    "    lstm_out = layers.LSTM(LSTM_UNITS)(td)\n",
    "\n",
    "    # The Dense layer is named \"cls_out\"\n",
    "    cls_out = layers.Dense(horizons * patch_h * patch_w, activation=\"sigmoid\", name=\"cls_out\")(lstm_out)\n",
    "    \n",
    "    # FIX: Remove the name from the Reshape layer\n",
    "    cls_out = layers.Reshape((horizons, patch_h, patch_w))(cls_out)\n",
    "\n",
    "    model = models.Model(inputs=inp, outputs=cls_out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "75d33401-4cf3-4d83-b873-78a55acf1e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorSpec(shape=(None, 6, 13, 13, 7), dtype=tf.float32, name=None), TensorSpec(shape=(None, 3, 13, 13), dtype=tf.float32, name=None))\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.element_spec)\n",
    "# as this none's are pointless , making it correct in the add_cls_batched()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e92766c1-8599-4ad5-8b12-7b77371b345b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bd2f64e9-74fb-4b7c-9ffc-de3555b0238c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_cnn_lstm_model(\n",
    "    seq_len=SEQ_LEN,\n",
    "    patch_h=PATCH_H,\n",
    "    patch_w=PATCH_W,\n",
    "    channels=CHANNELS,\n",
    "    horizons=HORIZONS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "040d3ca1-db4a-4d5d-ae17-2848bcbeaa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics=[tf.keras.metrics.BinaryAccuracy(), tf.keras.metrics.AUC()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f87cb8a8-b977-489f-a3cd-e8e4d93cad9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,            # stop if val_loss doesn't improve for 5 epochs\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "checkpoint = callbacks.ModelCheckpoint(\n",
    "    \"best_cnn_lstm_model.h5\",\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54aacb94-0770-44f8-bd4f-ea2f79b1cf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=1,\n",
    "    callbacks=[early_stop, checkpoint], \n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9064d8de-8857-4b78-a16a-af8e6b989f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Load the entire model from the .h5 file\n",
    "import tensorflow as tf \n",
    "model = tf.keras.models.load_model(r\"C:\\Users\\Ankit\\Downloads\\best_cnn_lstm_model (1).h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bec32e74-3809-4390-a353-9a64417ccfa2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)        │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ time_distributed (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │          <span style=\"color: #00af00; text-decoration-color: #00af00\">57,472</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">5,184</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ cls_out (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">507</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,619</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m7\u001b[0m)        │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ time_distributed (\u001b[38;5;33mTimeDistributed\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m64\u001b[0m)               │          \u001b[38;5;34m57,472\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                  │           \u001b[38;5;34m5,184\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ cls_out (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m507\u001b[0m)                 │           \u001b[38;5;34m8,619\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ reshape (\u001b[38;5;33mReshape\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">71,277</span> (278.43 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m71,277\u001b[0m (278.43 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">71,275</span> (278.42 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m71,275\u001b[0m (278.42 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> (12.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2\u001b[0m (12.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198eec3d-85bd-4676-92b9-5c367a433d34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
