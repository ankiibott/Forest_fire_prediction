{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "058e0175-ef36-46b5-b2b1-1d35d258311d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import tensorflow as tf\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "SEQ_LEN = 6                 \n",
    "HORIZONS = 3               \n",
    "PATCH_SIZE = 13             \n",
    "HALF = PATCH_SIZE // 2\n",
    "FILL_NAN_VALUE = 0.0\n",
    "\n",
    "REQUIRED_COLS = [\n",
    "    \"era5_t2m_file\", \"era5_d2m_file\", \"era5_tp_file\",\n",
    "    \"era5_u10_file\", \"era5_v10_file\",\n",
    "    \"viirs_file\", \"dem_file\", \"lulc_file\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3f5774f7-5a3f-4c9b-b70b-bfdfafab3ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.19.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "9ae142a5-292c-4193-9e60-d501771b46db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_single_raster(path):\n",
    "    with rasterio.open(path) as src:\n",
    "        arr = src.read()  # shape = (bands, H, W)\n",
    "\n",
    "    if arr.shape[0] == 1:\n",
    "        # single-band → squeeze to (H, W)\n",
    "        return arr[0]\n",
    "    else:\n",
    "        # multi-band (like VIIRS) → keep (bands, H, W)\n",
    "        return arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "38d95c89-2d72-4f83-897d-966405060ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rasters(df, raster_cols, max_workers=8):\n",
    "    \"\"\"Load unique rasters into memory, cached, with multithreading.\"\"\"\n",
    "    all_paths = set()\n",
    "\n",
    "    for col in raster_cols:\n",
    "        if col in df.columns:\n",
    "            all_paths.update(df[col].dropna().unique())\n",
    "    all_paths = list(all_paths)\n",
    "\n",
    "    cache = {}\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        results = list(ex.map(_load_single_raster, all_paths))\n",
    "\n",
    "    for path, arr in zip(all_paths, results):\n",
    "        if arr is not None:\n",
    "            cache[path] = arr   # ⚠️ for VIIRS this will be shape (bands, H, W)\n",
    "    return cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "f852d13a-0952-4b2c-b861-3443a07960e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_norm_stats(arrays):\n",
    "    \"\"\"Compute mean & std for normalization.\"\"\"\n",
    "    valid = np.concatenate([a[~np.isnan(a)].ravel() for a in arrays])\n",
    "    mean, std = valid.mean(), valid.std()\n",
    "    return mean, std\n",
    "\n",
    "def normalize(arr, mean, std):\n",
    "    return (arr - mean) / std\n",
    "\n",
    "def encode_lulc(lulc_arr, known_classes=None):\n",
    "    \"\"\"One-hot encode LULC raster.\"\"\"\n",
    "    flat = lulc_arr.ravel().astype(int).reshape(-1, 1)\n",
    "\n",
    "    enc = OneHotEncoder(categories=[known_classes] if known_classes else \"auto\", sparse=False)\n",
    "    onehot = enc.fit_transform(flat)\n",
    "\n",
    "    onehot_img = onehot.reshape(lulc_arr.shape[0], lulc_arr.shape[1], -1)\n",
    "    return onehot_img, enc.categories_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "edee5feb-0f88-4090-ab3f-4c28838c8f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _safe_center(h, w, patch_size=PATCH_SIZE):\n",
    "    half = patch_size // 2\n",
    "    r = np.clip(h // 2, half, h - half - 1)\n",
    "    c = np.clip(w // 2, half, w - half - 1)\n",
    "    return r, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "4fce6a62-85f9-4b91-a739-a40022b4718d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_patch(arr, row, col, patch_size=PATCH_SIZE):\n",
    "    half = patch_size // 2\n",
    "    h, w = arr.shape\n",
    "\n",
    "    r0 = row - half\n",
    "    r1 = row + half + 1\n",
    "    c0 = col - half\n",
    "    c1 = col + half + 1\n",
    "\n",
    "    # initialize empty patch with zeros\n",
    "    patch = np.zeros((patch_size, patch_size), dtype=arr.dtype)\n",
    "\n",
    "    # compute overlap between patch and array\n",
    "    r0_clip = max(r0, 0)\n",
    "    r1_clip = min(r1, h)\n",
    "    c0_clip = max(c0, 0)\n",
    "    c1_clip = min(c1, w)\n",
    "\n",
    "    # where to paste inside patch\n",
    "    pr0 = r0_clip - r0\n",
    "    pr1 = pr0 + (r1_clip - r0_clip)\n",
    "    pc0 = c0_clip - c0\n",
    "    pc1 = pc0 + (c1_clip - c0_clip)\n",
    "\n",
    "    # copy valid region\n",
    "    patch[pr0:pr1, pc0:pc1] = arr[r0_clip:r1_clip, c0_clip:c1_clip]\n",
    "\n",
    "    return patch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e402e30a-b656-4fe1-9eb9-99fc3e3171a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sample(seq_rows, horizon_rows, cache, force_fire=False):\n",
    "    seq_patches = []\n",
    "\n",
    "    # ---------- X sequence ----------\n",
    "    for _, row in seq_rows.iterrows():\n",
    "        bands = []\n",
    "        for var in [\"era5_t2m_file\", \"era5_d2m_file\", \"era5_tp_file\", \n",
    "                    \"era5_u10_file\", \"era5_v10_file\"]:\n",
    "            arr = cache[row[var]]          # (H, W)\n",
    "            h, w = arr.shape               # ✅ always 2D\n",
    "            r, c = _safe_center(h, w)\n",
    "            bands.append(_extract_patch(arr, r, c))\n",
    "\n",
    "        # static vars\n",
    "        dem = cache[row[\"dem_file\"]]       # (H, W)\n",
    "        lulc = cache[row[\"lulc_file\"]]     # (H, W)\n",
    "        h, w = dem.shape\n",
    "        r, c = _safe_center(h, w)\n",
    "        bands.append(_extract_patch(dem, r, c))\n",
    "        bands.append(_extract_patch(lulc, r, c))\n",
    "\n",
    "        seq_patches.append(np.stack(bands, axis=-1))\n",
    "\n",
    "    X = np.stack(seq_patches, axis=0)\n",
    "\n",
    "    # ---------- Y horizon ----------\n",
    "    horizon_patches = []\n",
    "    for _, row in horizon_rows.iterrows():\n",
    "        viirs_stack = cache[row[\"viirs_file\"]]   # (bands, H, W)\n",
    "        for idx in eval(row[\"target_band_idxs\"]):  # e.g. [9, 10, 11]\n",
    "            band = viirs_stack[idx-1]            # pick 2D slice (H, W)\n",
    "            h, w = band.shape\n",
    "            r, c = _safe_center(h, w)\n",
    "\n",
    "            if force_fire and np.any(band > 0):\n",
    "                fire_pos = np.argwhere(band > 0)\n",
    "                r, c = fire_pos[np.random.randint(len(fire_pos))]\n",
    "\n",
    "            horizon_patches.append(_extract_patch(band, r, c))\n",
    "\n",
    "    y = np.stack(horizon_patches, axis=0)\n",
    "\n",
    "    return X.astype(\"float32\"), y.astype(\"float32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed58a017-a51e-4db8-aa39-6fcd8b5befb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "afdf22cf-fdf9-4d61-920d-8afba9b072c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator(csv_path, cache, ensure_fire=True, fire_ratio=0.5):\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    for i in range(len(df) - SEQ_LEN - HORIZONS + 1):\n",
    "        seq_rows = df.iloc[i : i + SEQ_LEN]\n",
    "        horizon_rows = df.iloc[i + SEQ_LEN : i + SEQ_LEN + HORIZONS]\n",
    "\n",
    "        X, y = build_sample(seq_rows, horizon_rows, cache)  # ✅ no force_fire\n",
    "\n",
    "        # 🔍 Debug: count fires in target\n",
    "        fire_count = np.sum(y)\n",
    "        if fire_count > 0:\n",
    "            print(f\"🔥 Sample {i} contains {fire_count} fire pixels\")\n",
    "        yield X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "dbccc812-c0bf-4446-9bbe-8e352a0c2ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(csv_path, cache, shuffle=True, shuffle_buf=256,\n",
    "                   ensure_fire=True, fire_ratio=0.5):\n",
    "    output_signature = (\n",
    "        tf.TensorSpec(shape=(SEQ_LEN, PATCH_SIZE, PATCH_SIZE, 7), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(HORIZONS, PATCH_SIZE, PATCH_SIZE), dtype=tf.float32),\n",
    "    )\n",
    "    \n",
    "    ds = tf.data.Dataset.from_generator(\n",
    "        lambda: make_generator(csv_path, cache, ensure_fire=ensure_fire, fire_ratio=fire_ratio),\n",
    "        output_signature=output_signature\n",
    "    )\n",
    "    \n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(shuffle_buf, reshuffle_each_iteration=True)\n",
    "\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e327f91e-e454-4b6c-8ea4-f6b8b6aae545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading rasters into memory...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    csv_path = r\"C:\\Users\\Ankit\\Datasets_Forest_fire\\sequence_index_hourly_binary.csv\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    raster_cols = REQUIRED_COLS\n",
    "    print(\"Loading rasters into memory...\")\n",
    "    cache = load_rasters(df, raster_cols, max_workers=8)\n",
    "    print(f\"Loaded {len(cache)} rasters into memory ✅\")\n",
    "\n",
    "    # create dataset WITHOUT batching\n",
    "    ds = create_dataset(csv_path, cache)\n",
    "\n",
    "    # just check one sample (not batched yet)\n",
    "    for X, y in ds.take(1):\n",
    "        print(\"X shape:\", X.shape)  # (SEQ_LEN, PATCH, PATCH, 7)\n",
    "        print(\"y shape:\", y.shape)  # (HORIZONS, PATCH, PATCH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2982971f-a41a-4220-b7f0-afc46bf0b4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.EagerTensor'> <class 'tensorflow.python.framework.ops.EagerTensor'>\n"
     ]
    }
   ],
   "source": [
    "for X, y in ds.take(1):\n",
    "    print(type(X), type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7012220f-4bec-4b4f-b24a-20c111ff4128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples with fire: 0\n"
     ]
    }
   ],
   "source": [
    "fire_counts = []\n",
    "for i, (_, y) in enumerate(ds.take(500)):\n",
    "    fire_counts.append(np.sum(y.numpy()))\n",
    "\n",
    "print(\"Samples with fire:\", np.sum(np.array(fire_counts) > 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "103af332-026d-4a6f-9d6f-a11f17ef551b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL = 1000  \n",
    "VAL_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fce5a3e5-f7df-482e-851e-64a983e631e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.shuffle(1000, reshuffle_each_iteration=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4b1f3d67-7ef7-465c-a2a8-321ff13bfb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = int(TOTAL * VAL_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0be4f002-378a-4856-9040-4fe86aa5b5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = ds.take(val_size)\n",
    "train_dataset = ds.skip(val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3711b3f8-bdc6-41a5-9bab-91e406ada3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=2\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset   = val_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cf1c5c5c-a039-4939-b316-c9b3e4db71f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels: [0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "all_labels = []\n",
    "for _, y in val_dataset.take(10):  # just check 10 batches\n",
    "    all_labels.extend(y.numpy().flatten())\n",
    "\n",
    "print(\"Unique labels:\", np.unique(all_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "60e50094-9674-4b1a-b6f7-070789533b28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch labels: [[[[0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]]]]\n"
     ]
    }
   ],
   "source": [
    "for _, y in v_dataset.take(1):\n",
    "    print(\"Batch labels:\", y.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "02234fe6-6674-4d0d-9efb-a370d788a3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels found: [0.]\n",
      "Label 0.0: 8784789 samples\n",
      "⚠️ 1 is absent in labels!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "all_labels = []\n",
    "\n",
    "# Go through the entire dataset\n",
    "for _, y in train_dataset:  \n",
    "    all_labels.extend(y.numpy().flatten())\n",
    "\n",
    "unique_labels, counts = np.unique(all_labels, return_counts=True)\n",
    "\n",
    "print(\"Unique labels found:\", unique_labels)\n",
    "for lbl, cnt in zip(unique_labels, counts):\n",
    "    print(f\"Label {lbl}: {cnt} samples\")\n",
    "\n",
    "# Explicit checks\n",
    "if 0.0 not in unique_labels:\n",
    "    print(\"⚠️ 0 is absent in labels!\")\n",
    "if 1.0 not in unique_labels:\n",
    "    print(\"⚠️ 1 is absent in labels!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb904e44-5748-48d9-911a-0b37e234f794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 Band 9: 11 fire pixels\n",
      "🔥 Band 21: 1 fire pixels\n",
      "🔥 Band 23: 1 fire pixels\n",
      "🔥 Band 70: 1 fire pixels\n",
      "🔥 Band 105: 15 fire pixels\n",
      "🔥 Band 118: 2 fire pixels\n",
      "🔥 Band 129: 14 fire pixels\n",
      "🔥 Band 153: 6 fire pixels\n",
      "🔥 Band 176: 4 fire pixels\n",
      "🔥 Band 178: 1 fire pixels\n",
      "🔥 Band 190: 3 fire pixels\n",
      "🔥 Band 202: 7 fire pixels\n",
      "🔥 Band 214: 2 fire pixels\n",
      "🔥 Band 225: 24 fire pixels\n",
      "🔥 Band 238: 2 fire pixels\n",
      "🔥 Band 249: 6 fire pixels\n",
      "🔥 Band 261: 7 fire pixels\n",
      "🔥 Band 273: 42 fire pixels\n",
      "🔥 Band 285: 3 fire pixels\n",
      "🔥 Band 287: 2 fire pixels\n",
      "\n",
      "Total fire pixels (scanned so far): 154\n"
     ]
    }
   ],
   "source": [
    "import rasterio, numpy as np\n",
    "\n",
    "tif_path = r\"C:\\Users\\Ankit\\Datasets_Forest_fire\\VIIRS_fire_time_stack1.tif\"\n",
    "\n",
    "max_bands_to_show = 20  # only list first 20 fire bands\n",
    "found = 0\n",
    "total_fire_pixels = 0\n",
    "\n",
    "with rasterio.open(tif_path) as src:\n",
    "    for i in range(1, src.count + 1):\n",
    "        arr = src.read(i, masked=True)  # masked read is faster\n",
    "        fire_count = np.sum(arr == 1)\n",
    "        \n",
    "        if fire_count > 0:\n",
    "            print(f\"🔥 Band {i}: {fire_count} fire pixels\")\n",
    "            total_fire_pixels += fire_count\n",
    "            found += 1\n",
    "            if found >= max_bands_to_show:\n",
    "                break\n",
    "\n",
    "print(f\"\\nTotal fire pixels (scanned so far): {total_fire_pixels}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9029005f-08a1-4ff1-9766-8f8fdad7ba53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dea7aedd-d443-48ef-baa5-a60b6d741495",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2nd approach for the same dataset pipelining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edf08e23-1fb9-4974-8c8c-9e8f7aa7e3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import pandas as pd\n",
    "import random\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e7865da8-3220-49fa-8d23-16564f980994",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN     = 6\n",
    "HORIZONS    = [1, 2, 3]\n",
    "PATCH_SIZE  = 13  \n",
    "BATCH_SIZE  = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73a23609-a2ce-4f9f-8846-46de9eff05fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_CSV = r\"C:\\Users\\Ankit\\Datasets_Forest_fire\\sequence_index_hourly_norm.csv\"\n",
    "df = pd.read_csv(SEQ_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d011b055-9544-4b41-96b7-11b2fa396e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#as in the read_csv , it was stored as string \"[1,,2,3,4,5,6]\", so through ast.literal_eval we convert it into python list\n",
    "df[\"seq_band_idxs\"] = df[\"seq_band_idxs\"].apply(ast.literal_eval)\n",
    "df[\"target_band_idxs\"] = df[\"target_band_idxs\"].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42f826a1-ed62-4a49-93a8-7bf2cda29b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_patch(file, band_idx, row, col, size=PATCH_SIZE):\n",
    "    with rasterio.open(file) as src:\n",
    "        # Window (row, col) is center pixel\n",
    "        row_off = max(row - size // 2, 0)\n",
    "        col_off = max(col - size // 2, 0)\n",
    "        window = rasterio.windows.Window(col_off, row_off, size, size)\n",
    "        arr = src.read(band_idx+1, window=window)  # rasterio bands are 1-based\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "453cb015-54d7-4bb4-ad97-908ff60ef5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_generator():\n",
    "    while True:\n",
    "        \n",
    "        row = df.sample(1).iloc[0]\n",
    "\n",
    "        \n",
    "        with rasterio.open(row[\"era5_t2m_file\"]) as src:\n",
    "            h, w = src.height, src.width\n",
    "        r = random.randint(PATCH_SIZE//2, h - PATCH_SIZE//2 - 1)\n",
    "        c = random.randint(PATCH_SIZE//2, w - PATCH_SIZE//2 - 1)\n",
    "\n",
    "        \n",
    "        seq_bands = row[\"seq_band_idxs\"]\n",
    "        x_vars = []\n",
    "        for f in [\"era5_t2m_file\", \"era5_d2m_file\", \"era5_tp_file\", \n",
    "                  \"era5_u10_file\", \"era5_v10_file\"]:\n",
    "            var_stack = []\n",
    "            for b in seq_bands:\n",
    "                patch = read_patch(row[f], b, r, c)\n",
    "                var_stack.append(patch)\n",
    "            x_vars.append(np.stack(var_stack, axis=0))  # (time, H, W)\n",
    "\n",
    "        \n",
    "        dem_patch = read_patch(row[\"dem_file\"], 0, r, c)\n",
    "        x_vars.append(np.repeat(dem_patch[None, :, :], SEQ_LEN, axis=0))\n",
    "\n",
    "        \n",
    "        lulc_patch = read_patch(row[\"lulc_file\"], 0, r, c)\n",
    "        x_vars.append(np.repeat(lulc_patch[None, :, :], SEQ_LEN, axis=0))\n",
    "\n",
    "        x = np.stack(x_vars, axis=-1)  # shape: (time, H, W, channels)\n",
    "\n",
    "        \n",
    "        tgt_bands = row[\"target_band_idxs\"]\n",
    "        y_vars = []\n",
    "        for b in tgt_bands:\n",
    "            patch = read_patch(row[\"viirs_file\"], b, r, c)\n",
    "            y_vars.append(patch)\n",
    "        y = np.stack(y_vars, axis=0)  \n",
    "\n",
    "        yield x.astype(np.float32), y.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "add028ed-eec0-4ab8-b875-d11903e0fcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_signature = (\n",
    "    tf.TensorSpec(shape=(SEQ_LEN, PATCH_SIZE, PATCH_SIZE, None), dtype=tf.float32),\n",
    "    tf.TensorSpec(shape=(len(HORIZONS), PATCH_SIZE, PATCH_SIZE), dtype=tf.float32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee0dd7b8-56df-4e6f-8bcf-67118893addc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_generator(sample_generator, output_signature=output_signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ac89a0d-5967-4c2e-8eaf-a59d16298b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL = 1000  \n",
    "VAL_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4546dee1-3d53-46d5-9106-46661e48cf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(1000, reshuffle_each_iteration=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db6cab87-37a9-4cb3-9476-6224a906a082",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = int(TOTAL * VAL_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39a3c9ef-b7c9-4974-828d-2e8107cf4bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = dataset.take(val_size)\n",
    "train_dataset = dataset.skip(val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b7235e2b-7189-4cbb-9701-9bbaeebd0fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset   = val_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbd6271-29b2-4951-b20f-413cbaf2ccb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5fc03f-cd48-4ed7-bbca-f30245bc25ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90893721-b56d-4630-b86d-cf7a92ef7fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e700a567-9a37-440c-9e04-d691f0824fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 6           \n",
    "PATCH_H = 13            \n",
    "PATCH_W = 13          \n",
    "CHANNELS = 7       \n",
    "HORIZONS = 3            \n",
    "LSTM_UNITS = 16      \n",
    "CNN_FEATURES = 64 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27354090-4f0c-46e7-a8d1-0d0897ea2f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_lstm_model(seq_len=SEQ_LEN, patch_h=PATCH_H, patch_w=PATCH_W, channels=CHANNELS, horizons=HORIZONS):\n",
    "\n",
    "    inp = layers.Input(shape=(seq_len, patch_h, patch_w, channels))\n",
    "\n",
    "    def build_cnn_block():\n",
    "        model = models.Sequential([\n",
    "            layers.Conv2D(32, (3,3), activation='relu', padding='same'),\n",
    "            layers.MaxPooling2D((2,2)),\n",
    "            layers.Conv2D(64, (3,3), activation='relu', padding='same'),\n",
    "            layers.MaxPooling2D((2,2)),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(CNN_FEATURES, activation='relu')\n",
    "        ])\n",
    "        return model\n",
    "\n",
    "    cnn = build_cnn_block()\n",
    "    td = layers.TimeDistributed(cnn)(inp) \n",
    "    \n",
    "    lstm_out = layers.LSTM(LSTM_UNITS)(td)\n",
    "\n",
    "    #for spatial features as an output\n",
    "    reg_out = layers.Dense(horizons * patch_h * patch_w, activation=\"linear\")(lstm_out)\n",
    "    reg_out = layers.Reshape((horizons, patch_h, patch_w),name=\"reg_out\")(reg_out)\n",
    "\n",
    "    #fire/no fire\n",
    "    cls_out = layers.Dense(1, activation=\"sigmoid\",name=\"cls_out\")(lstm_out)  \n",
    "\n",
    "    model = models.Model(inputs=inp, outputs=[reg_out, cls_out])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "75d33401-4cf3-4d83-b873-78a55acf1e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorSpec(shape=(None, None, 6, 13, 13, 7), dtype=tf.float32, name=None), {'reg_out': TensorSpec(shape=(None, None, 3, 13, 13), dtype=tf.float32, name=None), 'cls_out': TensorSpec(shape=(None, None, 1), dtype=tf.float32, name=None)})\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.element_spec)\n",
    "# as this none's are pointless , making it correct in the add_cls_batched()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0edd5396-3de9-4d23-9be3-633aa055e861",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "FIRE_THRESHOLD = 0.5\n",
    "\n",
    "def add_cls_label_batched(X, y):\n",
    "    \n",
    "    y_reg = y['reg_out']  \n",
    "    y_reg_shape = tf.shape(y_reg)\n",
    "    \n",
    "    \n",
    "    leading_dims = tf.reduce_prod(y_reg_shape[:-3])\n",
    "    y_reg_flat = tf.reshape(y_reg, (leading_dims, y_reg_shape[-3], y_reg_shape[-2], y_reg_shape[-1]))\n",
    "    \n",
    "    \n",
    "    y_cls = tf.reduce_max(y_reg_flat, axis=[1,2,3])\n",
    "    y_cls = tf.cast(y_cls > FIRE_THRESHOLD, tf.float32)\n",
    "    y_cls = tf.expand_dims(y_cls, axis=-1)\n",
    "    \n",
    "   \n",
    "    X_shape = tf.shape(X)\n",
    "    X_flat = tf.reshape(X, (leading_dims, X_shape[-4], X_shape[-3], X_shape[-2], X_shape[-1]))\n",
    "\n",
    "    return X_flat, {\"reg_out\": y_reg_flat, \"cls_out\": y_cls}\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(add_cls_label_batched, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_dataset   = val_dataset.map(add_cls_label_batched, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e92766c1-8599-4ad5-8b12-7b77371b345b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bd2f64e9-74fb-4b7c-9ffc-de3555b0238c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_cnn_lstm_model(\n",
    "    seq_len=SEQ_LEN,\n",
    "    patch_h=PATCH_H,\n",
    "    patch_w=PATCH_W,\n",
    "    channels=CHANNELS,\n",
    "    horizons=HORIZONS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "040d3ca1-db4a-4d5d-ae17-2848bcbeaa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "    loss={\n",
    "        \"reg_out\": tf.keras.losses.MeanSquaredError(),\n",
    "        \"cls_out\": tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "    },\n",
    "    loss_weights={\"reg_out\": 1.0, \"cls_out\": 0.3},\n",
    "    metrics={\"reg_out\": [tf.keras.metrics.MeanAbsoluteError()],\n",
    "             \"cls_out\": [tf.keras.metrics.BinaryAccuracy(), tf.keras.metrics.AUC()]},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f87cb8a8-b977-489f-a3cd-e8e4d93cad9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,            # stop if val_loss doesn't improve for 5 epochs\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "checkpoint = callbacks.ModelCheckpoint(\n",
    "    \"best_cnn_lstm_model.h5\",\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54aacb94-0770-44f8-bd4f-ea2f79b1cf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=1,\n",
    "    callbacks=[early_stop, checkpoint], \n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9064d8de-8857-4b78-a16a-af8e6b989f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
