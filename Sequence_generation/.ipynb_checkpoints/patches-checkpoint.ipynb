{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17b97c66-082d-4a6e-93a8-fdab3c13be54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} ValueError: CSV missing columns: ['lulc2015_file', 'lulc2016_file']\nTraceback (most recent call last):\n\n  File \"C:\\Users\\Ankit\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 269, in __call__\n    ret = func(*args)\n\n  File \"C:\\Users\\Ankit\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n\n  File \"C:\\Users\\Ankit\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\data\\ops\\from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"C:\\Users\\Ankit\\AppData\\Local\\Temp\\ipykernel_16404\\2317204656.py\", line 97, in generator\n    raise ValueError(f\"CSV missing columns: {missing}\")\n\nValueError: CSV missing columns: ['lulc2015_file', 'lulc2016_file']\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNext] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 127\u001b[0m\n\u001b[0;32m    125\u001b[0m csv_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mAnkit\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDatasets_Forest_fire\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124msequence_index_hourly.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m   \u001b[38;5;66;03m# <-- put your CSV here\u001b[39;00m\n\u001b[0;32m    126\u001b[0m ds \u001b[38;5;241m=\u001b[39m create_dataset(csv_path, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 127\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m ds\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, X\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# (B, SEQ_LEN, PATCH, PATCH, 8)\u001b[39;00m\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, y\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# (B, HORIZONS, PATCH, PATCH)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:826\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    825\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 826\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    827\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOutOfRangeError:\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:776\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[38;5;66;03m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[0;32m    774\u001b[0m \u001b[38;5;66;03m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[0;32m    775\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecution_mode(context\u001b[38;5;241m.\u001b[39mSYNC):\n\u001b[1;32m--> 776\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator_get_next\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    777\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    778\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    781\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;66;03m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_spec\u001b[38;5;241m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3086\u001b[0m, in \u001b[0;36miterator_get_next\u001b[1;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   3084\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   3085\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 3086\u001b[0m   \u001b[43m_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from_not_ok_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3087\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_FallbackException:\n\u001b[0;32m   3088\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:6006\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   6004\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[0;32m   6005\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m-> 6006\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} ValueError: CSV missing columns: ['lulc2015_file', 'lulc2016_file']\nTraceback (most recent call last):\n\n  File \"C:\\Users\\Ankit\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 269, in __call__\n    ret = func(*args)\n\n  File \"C:\\Users\\Ankit\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n\n  File \"C:\\Users\\Ankit\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\data\\ops\\from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"C:\\Users\\Ankit\\AppData\\Local\\Temp\\ipykernel_16404\\2317204656.py\", line 97, in generator\n    raise ValueError(f\"CSV missing columns: {missing}\")\n\nValueError: CSV missing columns: ['lulc2015_file', 'lulc2016_file']\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNext] name: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import tensorflow as tf\n",
    "\n",
    "# =====================\n",
    "# CONFIG\n",
    "# =====================\n",
    "SEQ_LEN = 6                # past 6 timesteps\n",
    "HORIZONS = 3               # predict next 3 hours\n",
    "PATCH_SIZE = 33            # patch size\n",
    "HALF = PATCH_SIZE // 2\n",
    "\n",
    "# CSV must have at least these columns\n",
    "REQUIRED = [\n",
    "    \"center_time\", \"seq_band_idxs\", \"target_band_idxs\",\n",
    "    \"era5_t2m_file\", \"era5_d2m_file\", \"era5_tp_file\",\n",
    "    \"era5_u10_file\", \"era5_v10_file\",\n",
    "    \"viirs_file\",\n",
    "    \"dem_file\", \"lulc2015_file\", \"lulc2016_file\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f347aab9-6020-4b97-9b1c-997b10aaa55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_safe_center(h, w, patch_size=PATCH_SIZE):\n",
    "    half = patch_size // 2\n",
    "    r = h // 2\n",
    "    c = w // 2\n",
    "    r = np.clip(r, half, h - half - 1)\n",
    "    c = np.clip(c, half, w - half - 1)\n",
    "    return r, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c77676b-992c-4482-b8de-8e0ce71c4a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patch(raster_path, r, c, patch_size=PATCH_SIZE):\n",
    "    with rasterio.open(raster_path) as src:\n",
    "        H, W = src.height, src.width\n",
    "        if H < patch_size or W < patch_size:\n",
    "            raise ValueError(f\"Raster {raster_path} smaller than patch size.\")\n",
    "        window = rasterio.windows.Window(c - patch_size//2, r - patch_size//2,\n",
    "                                         patch_size, patch_size)\n",
    "        patch = src.read(1, window=window).astype(np.float32)\n",
    "        nodata = src.nodata\n",
    "        if nodata is not None:\n",
    "            patch = np.where(patch == nodata, np.nan, patch)\n",
    "    return patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee14c06-4d06-46da-85fa-67e6ffad2d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sample(row, seq_len=SEQ_LEN, horizons=HORIZONS, patch_size=PATCH_SIZE):\n",
    "    # Open one raster to get safe center\n",
    "    with rasterio.open(row[\"era5_t2m_file\"]) as src:\n",
    "        H, W = src.height, src.width\n",
    "    r, c = get_safe_center(H, W, patch_size)\n",
    "\n",
    "    # Sequence ERA5 (5 vars × seq_len)\n",
    "    seq_patches = []\n",
    "    for fcol in [\"era5_t2m_file\", \"era5_d2m_file\", \"era5_tp_file\",\n",
    "                 \"era5_u10_file\", \"era5_v10_file\"]:\n",
    "        patch = extract_patch(row[fcol], r, c, patch_size)\n",
    "        seq_patches.append(patch)\n",
    "    seq_stack = np.stack(seq_patches, axis=-1)  # (H, W, 5)\n",
    "\n",
    "    # Static DEM\n",
    "    dem_patch = extract_patch(row[\"dem_file\"], r, c, patch_size)[..., None]\n",
    "\n",
    "    # Static LULC 2015 + 2016\n",
    "    lulc15 = extract_patch(row[\"lulc2015_file\"], r, c, patch_size)[..., None]\n",
    "    lulc16 = extract_patch(row[\"lulc2016_file\"], r, c, patch_size)[..., None]\n",
    "\n",
    "    # Combine static → (H, W, 3)\n",
    "    static_stack = np.concatenate([dem_patch, lulc15, lulc16], axis=-1)\n",
    "\n",
    "    # Expand static to each timestep → (seq_len, H, W, 3)\n",
    "    static_seq = np.repeat(static_stack[None, ...], seq_len, axis=0)\n",
    "\n",
    "    # Final input X = (seq_len, H, W, channels=8)\n",
    "    X = np.concatenate([np.repeat(seq_stack[None, ...], seq_len, axis=0), static_seq], axis=-1)\n",
    "\n",
    "    # Target VIIRS (3 horizons)\n",
    "    target_patch = extract_patch(row[\"viirs_file\"], r, c, patch_size)\n",
    "    y = np.repeat(target_patch[None, ...], horizons, axis=0)  # (HORIZONS, H, W)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8efb2d-fe35-4d1b-a1e4-67bae49efea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    missing = [c for c in REQUIRED if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"CSV missing columns: {missing}\")\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        try:\n",
    "            X, y = extract_sample(row)\n",
    "            yield X.astype(np.float32), y.astype(np.float32)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping row due to error: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0113892f-10cd-4e1e-ac4b-2eb50b94cd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(csv_path, batch_size=4, shuffle=True):\n",
    "    output_signature = (\n",
    "        tf.TensorSpec(shape=(SEQ_LEN, PATCH_SIZE, PATCH_SIZE, 8), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(HORIZONS, PATCH_SIZE, PATCH_SIZE), dtype=tf.float32),\n",
    "    )\n",
    "    ds = tf.data.Dataset.from_generator(\n",
    "        lambda: generator(csv_path),\n",
    "        output_signature=output_signature\n",
    "    )\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(100)\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e89f78c-e8f8-4824-b138-960e2f8e7826",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    csv_path = r\"C:\\Users\\Ankit\\Datasets_Forest_fire\\sequence_index_hourly.csv\"   # <-- put your CSV here\n",
    "    ds = create_dataset(csv_path, batch_size=2)\n",
    "    for X, y in ds.take(1):\n",
    "        print(\"X shape:\", X.shape)  # (B, SEQ_LEN, PATCH, PATCH, 8)\n",
    "        print(\"y shape:\", y.shape)  # (B, HORIZONS, PATCH, PATCH)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
