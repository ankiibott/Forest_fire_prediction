{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "058e0175-ef36-46b5-b2b1-1d35d258311d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import tensorflow as tf\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# =====================\n",
    "# CONFIG\n",
    "# =====================\n",
    "SEQ_LEN = 6                 # number of past timesteps to use\n",
    "HORIZONS = 3                # number of future timesteps to predict\n",
    "PATCH_SIZE = 13             # spatial patch size\n",
    "HALF = PATCH_SIZE // 2\n",
    "FILL_NAN_VALUE = 0.0\n",
    "\n",
    "REQUIRED_COLS = [\n",
    "    \"era5_t2m_file\", \"era5_d2m_file\", \"era5_tp_file\",\n",
    "    \"era5_u10_file\", \"era5_v10_file\",\n",
    "    \"viirs_file\", \"dem_file\", \"lulc_file\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ae142a5-292c-4193-9e60-d501771b46db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_single_raster(path):\n",
    "    \"\"\"Helper to load one raster efficiently (single band).\"\"\"\n",
    "    try:\n",
    "        with rasterio.open(path, sharing=True) as src:\n",
    "            arr = src.read(1, out_dtype=\"float32\", masked=True)  # read first band\n",
    "            if np.ma.is_masked(arr):\n",
    "                arr = arr.filled(np.nan)\n",
    "            arr = np.nan_to_num(arr, nan=FILL_NAN_VALUE).astype(\"float32\", copy=False)\n",
    "            return arr\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error reading {path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38d95c89-2d72-4f83-897d-966405060ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rasters(df, raster_cols, max_workers=8):\n",
    "    \"\"\"Load unique rasters into memory, cached, with multithreading.\"\"\"\n",
    "    all_paths = set()\n",
    "    for col in raster_cols:\n",
    "        if col in df.columns:\n",
    "            all_paths.update(df[col].dropna().unique())\n",
    "    all_paths = list(all_paths)\n",
    "\n",
    "    cache = {}\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        results = list(ex.map(_load_single_raster, all_paths))\n",
    "\n",
    "    for path, arr in zip(all_paths, results):\n",
    "        if arr is not None:\n",
    "            cache[path] = arr\n",
    "    return cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f852d13a-0952-4b2c-b861-3443a07960e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_norm_stats(arrays):\n",
    "    \"\"\"Compute mean & std for normalization.\"\"\"\n",
    "    valid = np.concatenate([a[~np.isnan(a)].ravel() for a in arrays])\n",
    "    mean, std = valid.mean(), valid.std()\n",
    "    return mean, std\n",
    "\n",
    "def normalize(arr, mean, std):\n",
    "    return (arr - mean) / std\n",
    "\n",
    "def encode_lulc(lulc_arr, known_classes=None):\n",
    "    \"\"\"One-hot encode LULC raster.\"\"\"\n",
    "    flat = lulc_arr.ravel().astype(int).reshape(-1, 1)\n",
    "\n",
    "    enc = OneHotEncoder(categories=[known_classes] if known_classes else \"auto\", sparse=False)\n",
    "    onehot = enc.fit_transform(flat)\n",
    "\n",
    "    onehot_img = onehot.reshape(lulc_arr.shape[0], lulc_arr.shape[1], -1)\n",
    "    return onehot_img, enc.categories_[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edee5feb-0f88-4090-ab3f-4c28838c8f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _safe_center(h, w, patch_size=PATCH_SIZE):\n",
    "    half = patch_size // 2\n",
    "    r = np.clip(h // 2, half, h - half - 1)\n",
    "    c = np.clip(w // 2, half, w - half - 1)\n",
    "    return r, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fce6a62-85f9-4b91-a739-a40022b4718d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_patch(arr, row, col, patch_size=PATCH_SIZE):\n",
    "    half = patch_size // 2\n",
    "    return arr[row-half:row+half+1, col-half:col+half+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e402e30a-b656-4fe1-9eb9-99fc3e3171a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sample(seq_rows, horizon_rows, cache):\n",
    "    \"\"\"\n",
    "    Build one training sample:\n",
    "      seq_rows: past SEQ_LEN rows from CSV\n",
    "      horizon_rows: next HORIZONS rows from CSV\n",
    "    Returns:\n",
    "      X: (SEQ_LEN, PATCH_SIZE, PATCH_SIZE, 7)\n",
    "      y: (HORIZONS, PATCH_SIZE, PATCH_SIZE)\n",
    "    \"\"\"\n",
    "    seq_patches = []\n",
    "    # --- sequence input ---\n",
    "    for _, row in seq_rows.iterrows():\n",
    "        bands = []\n",
    "        for var in [\"era5_t2m_file\", \"era5_d2m_file\", \"era5_tp_file\", \"era5_u10_file\", \"era5_v10_file\"]:\n",
    "            arr = cache[row[var]]\n",
    "            r, c = _safe_center(*arr.shape)\n",
    "            bands.append(_extract_patch(arr, r, c))\n",
    "        # static vars\n",
    "        dem = cache[row[\"dem_file\"]]\n",
    "        lulc = cache[row[\"lulc_file\"]]\n",
    "        r, c = _safe_center(*dem.shape)\n",
    "        dem_patch = _extract_patch(dem, r, c)\n",
    "        lulc_patch = _extract_patch(lulc, r, c)\n",
    "        bands.append(dem_patch)\n",
    "        bands.append(lulc_patch)\n",
    "\n",
    "        stack = np.stack(bands, axis=-1)  # (P, P, 7)\n",
    "        seq_patches.append(stack)\n",
    "\n",
    "    X = np.stack(seq_patches, axis=0)  # (SEQ_LEN, P, P, 7)\n",
    "\n",
    "    # --- horizon target ---\n",
    "    horizon_patches = []\n",
    "    for _, row in horizon_rows.iterrows():\n",
    "        arr = cache[row[\"viirs_file\"]]\n",
    "        r, c = _safe_center(*arr.shape)\n",
    "        horizon_patches.append(_extract_patch(arr, r, c))\n",
    "    y = np.stack(horizon_patches, axis=0)  # (HORIZONS, P, P)\n",
    "\n",
    "    return X.astype(\"float32\"), y.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afdf22cf-fdf9-4d61-920d-8afba9b072c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator(csv_path, cache):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    missing = [c for c in REQUIRED_COLS if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"CSV missing required columns: {missing}\")\n",
    "\n",
    "    for i in range(len(df) - SEQ_LEN - HORIZONS + 1):\n",
    "        seq_rows = df.iloc[i : i + SEQ_LEN]\n",
    "        horizon_rows = df.iloc[i + SEQ_LEN : i + SEQ_LEN + HORIZONS]\n",
    "        yield build_sample(seq_rows, horizon_rows, cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbccc812-c0bf-4446-9bbe-8e352a0c2ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(csv_path, cache, batch_size=4, shuffle=True, shuffle_buf=256):\n",
    "    output_signature = (\n",
    "        tf.TensorSpec(shape=(SEQ_LEN, PATCH_SIZE, PATCH_SIZE, 7), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(HORIZONS, PATCH_SIZE, PATCH_SIZE), dtype=tf.float32),\n",
    "    )\n",
    "    ds = tf.data.Dataset.from_generator(\n",
    "        lambda: make_generator(csv_path, cache),\n",
    "        output_signature=output_signature\n",
    "    )\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(shuffle_buf, reshuffle_each_iteration=True)\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e327f91e-e454-4b6c-8ea4-f6b8b6aae545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading rasters into memory...\n",
      "Loaded 9 rasters into memory ✅\n",
      "X shape: (2, 6, 13, 13, 7)\n",
      "y shape: (2, 3, 13, 13)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    csv_path = r\"C:\\Users\\Ankit\\Datasets_Forest_fire\\sequence_index_hourly_right.csv\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    raster_cols = REQUIRED_COLS\n",
    "    print(\"Loading rasters into memory...\")\n",
    "    cache = load_rasters(df, raster_cols, max_workers=8)\n",
    "    print(f\"Loaded {len(cache)} rasters into memory ✅\")\n",
    "\n",
    "    ds = create_dataset(csv_path, cache, batch_size=2)\n",
    "    for X, y in ds.take(1):\n",
    "        print(\"X shape:\", X.shape)  # (B, SEQ_LEN, PATCH, PATCH, 7)\n",
    "        print(\"y shape:\", y.shape)  # (B, HORIZONS, PATCH, PATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2982971f-a41a-4220-b7f0-afc46bf0b4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.EagerTensor'> <class 'tensorflow.python.framework.ops.EagerTensor'>\n"
     ]
    }
   ],
   "source": [
    "for X, y in ds.take(1):\n",
    "    print(type(X), type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf08e23-1fb9-4974-8c8c-9e8f7aa7e3a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
