{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "058e0175-ef36-46b5-b2b1-1d35d258311d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import tensorflow as tf\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "SEQ_LEN = 6                 \n",
    "HORIZONS = 3               \n",
    "PATCH_SIZE = 13             \n",
    "HALF = PATCH_SIZE // 2\n",
    "FILL_NAN_VALUE = 0.0\n",
    "\n",
    "REQUIRED_COLS = [\n",
    "    \"era5_t2m_file\", \"era5_d2m_file\", \"era5_tp_file\",\n",
    "    \"era5_u10_file\", \"era5_v10_file\",\n",
    "    \"viirs_file\", \"dem_file\", \"lulc_file\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f5774f7-5a3f-4c9b-b70b-bfdfafab3ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.20.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ae142a5-292c-4193-9e60-d501771b46db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_single_raster(path):\n",
    "    with rasterio.open(path) as src:\n",
    "        arr = src.read() \n",
    "\n",
    "    if arr.shape[0] == 1:\n",
    "        \n",
    "        return arr[0]\n",
    "    else:\n",
    "     \n",
    "        return arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38d95c89-2d72-4f83-897d-966405060ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rasters(df, raster_cols, max_workers=8):\n",
    "    \n",
    "    all_paths = set()\n",
    "\n",
    "    for col in raster_cols:\n",
    "        if col in df.columns:\n",
    "            all_paths.update(df[col].dropna().unique())\n",
    "    all_paths = list(all_paths)\n",
    "\n",
    "    cache = {}\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        results = list(ex.map(_load_single_raster, all_paths))\n",
    "\n",
    "    for path, arr in zip(all_paths, results):\n",
    "        if arr is not None:\n",
    "            cache[path] = arr\n",
    "    return cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edee5feb-0f88-4090-ab3f-4c28838c8f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _safe_center(h, w, patch_size=PATCH_SIZE):\n",
    "    half = patch_size // 2\n",
    "    r = np.clip(h // 2, half, h - half - 1)\n",
    "    c = np.clip(w // 2, half, w - half - 1)\n",
    "    return r, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fce6a62-85f9-4b91-a739-a40022b4718d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_patch(arr, row, col, patch_size=PATCH_SIZE):\n",
    "    half = patch_size // 2\n",
    "    h, w = arr.shape\n",
    "\n",
    "    r0 = row - half\n",
    "    r1 = row + half + 1\n",
    "    c0 = col - half\n",
    "    c1 = col + half + 1\n",
    "\n",
    "    patch = np.zeros((patch_size, patch_size), dtype=arr.dtype)\n",
    "\n",
    "    r0_clip = max(r0, 0)\n",
    "    r1_clip = min(r1, h)\n",
    "    c0_clip = max(c0, 0)\n",
    "    c1_clip = min(c1, w)\n",
    "\n",
    "    pr0 = r0_clip - r0\n",
    "    pr1 = pr0 + (r1_clip - r0_clip)\n",
    "    pc0 = c0_clip - c0\n",
    "    pc1 = pc0 + (c1_clip - c0_clip)\n",
    "\n",
    "    patch[pr0:pr1, pc0:pc1] = arr[r0_clip:r1_clip, c0_clip:c1_clip]\n",
    "\n",
    "    return patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e402e30a-b656-4fe1-9eb9-99fc3e3171a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sample(seq_rows, horizon_rows, cache, force_fire=False):\n",
    "    seq_patches = []\n",
    "\n",
    "   \n",
    "    for _, row in seq_rows.iterrows():\n",
    "        bands = []\n",
    "        for var in [\"era5_t2m_file\", \"era5_d2m_file\", \"era5_tp_file\",\n",
    "                    \"era5_u10_file\", \"era5_v10_file\"]:\n",
    "            arr = cache[row[var]]\n",
    "\n",
    "            if len(arr.shape) == 3:\n",
    "                arr = arr[0]\n",
    "\n",
    "            h, w = arr.shape\n",
    "            r, c = _safe_center(h, w)\n",
    "            bands.append(_extract_patch(arr, r, c))\n",
    "\n",
    "        dem = cache[row[\"dem_file\"]]\n",
    "        lulc = cache[row[\"lulc_file\"]]\n",
    "\n",
    "        if len(dem.shape) == 3:\n",
    "            dem = dem[0]\n",
    "        if len(lulc.shape) == 3:\n",
    "            lulc = lulc[0]\n",
    "\n",
    "        h, w = dem.shape\n",
    "        r, c = _safe_center(h, w)\n",
    "        bands.append(_extract_patch(dem, r, c))\n",
    "        bands.append(_extract_patch(lulc, r, c))\n",
    "\n",
    "        seq_patches.append(np.stack(bands, axis=-1))\n",
    "\n",
    "    X = np.stack(seq_patches, axis=0)\n",
    "\n",
    "\n",
    "    horizon_patches = []\n",
    "   \n",
    "    for _, row in horizon_rows.iterrows():\n",
    "        viirs_stack = cache[row[\"viirs_file\"]]\n",
    "        \n",
    "    \n",
    "        target_band_idx_list = eval(row[\"target_band_idxs\"])\n",
    " \n",
    "        idx = target_band_idx_list[0]\n",
    "        \n",
    "        band = viirs_stack[idx - 1]\n",
    "        h, w = band.shape\n",
    "        r, c = _safe_center(h, w)\n",
    "\n",
    "        if force_fire and np.any(band > 0):\n",
    "            fire_pos = np.argwhere(band > 0)\n",
    "            r, c = fire_pos[np.random.randint(len(fire_pos))]\n",
    "\n",
    "        horizon_patches.append(_extract_patch(band, r, c))\n",
    "\n",
    "    y = np.stack(horizon_patches, axis=0)\n",
    "\n",
    "    return X.astype(\"float32\"), y.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed58a017-a51e-4db8-aa39-6fcd8b5befb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import tensorflow as tf\n",
    "\n",
    "# def make_generator(df, cache, fire_ratio=0.5):\n",
    "    \n",
    "#     valid_start_indices = list(range(len(df) - SEQ_LEN - HORIZONS + 1))\n",
    "\n",
    "#     fire_start_indices = []\n",
    "#     non_fire_start_indices = []\n",
    "    \n",
    "#     for i in valid_start_indices:\n",
    "#         horizon_rows = df.iloc[i + SEQ_LEN : i + SEQ_LEN + HORIZONS]\n",
    "#         # This line iterates through DataFrame rows to check for fire, which is fine.\n",
    "#         has_fire = any(np.any(cache[row[\"viirs_file\"]] > 0) for _, row in horizon_rows.iterrows())\n",
    "        \n",
    "#         if has_fire:\n",
    "#             fire_start_indices.append(i)\n",
    "#         else:\n",
    "#             non_fire_start_indices.append(i)\n",
    "\n",
    "#     # --- Start of sampling logic (your core strategy) ---\n",
    "#     num_fire_samples = len(fire_start_indices)\n",
    "    \n",
    "#     if num_fire_samples == 0:\n",
    "#         print(\"Warning: No fire events found in the dataset. Training will be difficult.\")\n",
    "#         # If no fires, take a small sample of non-fire indices\n",
    "#         num_non_fire_samples_to_use = min(len(non_fire_start_indices), 1000)\n",
    "#     else:\n",
    "#         # Calculate how many non-fire samples to use to achieve the desired ratio\n",
    "#         num_non_fire_samples_to_use = int((num_fire_samples / fire_ratio) - num_fire_samples)\n",
    "#         num_non_fire_samples_to_use = min(num_non_fire_samples_to_use, len(non_fire_start_indices))\n",
    "\n",
    "#     # Sample the indices to create a balanced list\n",
    "#     # The crucial change is ensuring all indices are integers.\n",
    "#     fire_indices_to_use = np.array(fire_start_indices, dtype=int)\n",
    "#     non_fire_indices_to_use = np.random.choice(\n",
    "#         non_fire_start_indices,\n",
    "#         size=num_non_fire_samples_to_use,\n",
    "#         replace=False\n",
    "#     ).astype(int)  # Ensure non-fire indices are also integers\n",
    "    \n",
    "#     indices_to_use = np.concatenate([fire_indices_to_use, non_fire_indices_to_use])\n",
    "#     np.random.shuffle(indices_to_use)\n",
    "    \n",
    "#     # --- End of sampling logic ---\n",
    "    \n",
    "#     for i in indices_to_use:\n",
    "#         # 'i' is now guaranteed to be an integer, preventing the TypeError\n",
    "#         seq_rows = df.iloc[i : i + SEQ_LEN]\n",
    "#         horizon_rows = df.iloc[i + SEQ_LEN : i + SEQ_LEN + HORIZONS]\n",
    "        \n",
    "#         X, y = build_sample(seq_rows, horizon_rows, cache)\n",
    "        \n",
    "#         yield X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "afdf22cf-fdf9-4d61-920d-8afba9b072c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_generator(df, cache, fire_ratio=0.5):\n",
    "    \n",
    "#     valid_start_indices = list(range(len(df) - SEQ_LEN - HORIZONS + 1))\n",
    "\n",
    "#     fire_start_indices = []\n",
    "#     non_fire_start_indices = []\n",
    "    \n",
    "#     for i in valid_start_indices:\n",
    "#         horizon_rows = df.iloc[i + SEQ_LEN : i + SEQ_LEN + HORIZONS]\n",
    "#         has_fire = any(np.any(cache[row[\"viirs_file\"]] > 0) for _, row in horizon_rows.iterrows())\n",
    "        \n",
    "#         if has_fire:\n",
    "#             fire_start_indices.append(i)\n",
    "#         else:\n",
    "#             non_fire_start_indices.append(i)\n",
    "\n",
    "    \n",
    "#     num_fire_samples = len(fire_start_indices)\n",
    "    \n",
    "#     if num_fire_samples == 0:\n",
    "#         print(\"Warning: No fire events found in the dataset. Training will be difficult.\")\n",
    "        \n",
    "#         num_non_fire_samples_to_use = min(len(non_fire_start_indices), 1000) \n",
    "#     else:\n",
    "   \n",
    "#         num_non_fire_samples_to_use = int((num_fire_samples / fire_ratio) - num_fire_samples)\n",
    "#         num_non_fire_samples_to_use = min(num_non_fire_samples_to_use, len(non_fire_start_indices))\n",
    "\n",
    "#     # 4. Sample the indices to create a balanced list\n",
    "#     fire_indices_to_use = fire_start_indices\n",
    "#     non_fire_indices_to_use = np.random.choice(\n",
    "#         non_fire_start_indices,\n",
    "#         size=num_non_fire_samples_to_use,\n",
    "#         replace=False \n",
    "#     )\n",
    "    \n",
    "#     indices_to_use = np.concatenate([fire_indices_to_use, non_fire_indices_to_use])\n",
    "#     np.random.shuffle(indices_to_use)\n",
    "#     indices_to_use = indices_to_use.astype(int) # Add this line to fix the type\n",
    "    \n",
    "#     for i in indices_to_use:\n",
    "#         seq_rows = df.iloc[i : i + SEQ_LEN]\n",
    "#         horizon_rows = df.iloc[i + SEQ_LEN : i + SEQ_LEN + HORIZONS]\n",
    "\n",
    "        \n",
    "#         X, y = build_sample(seq_rows, horizon_rows, cache)\n",
    "        \n",
    "#         yield X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32da5b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator(df, cache, fire_ratio=0.5):\n",
    "    \"\"\"\n",
    "    Generates balanced samples of sequences with and without fire events.\n",
    "    \"\"\"\n",
    "    valid_start_indices = list(range(len(df) - SEQ_LEN - HORIZONS + 1))\n",
    "    fire_start_indices = []\n",
    "    non_fire_start_indices = []\n",
    "    \n",
    "    print(\"Scanning data for fire and non-fire events...\")\n",
    "    for i in valid_start_indices:\n",
    "        horizon_rows = df.iloc[i + SEQ_LEN : i + SEQ_LEN + HORIZONS]\n",
    "        has_fire = any(np.any(cache[row[\"viirs_file\"]] > 0) for _, row in horizon_rows.iterrows())\n",
    "        \n",
    "        if has_fire:\n",
    "            fire_start_indices.append(i)\n",
    "        else:\n",
    "            non_fire_start_indices.append(i)\n",
    "\n",
    "    num_fire_samples = len(fire_start_indices)\n",
    "    \n",
    "    if num_fire_samples == 0:\n",
    "        print(\"⚠️ Warning: No fire events found in the dataset.\")\n",
    "        num_non_fire_samples_to_use = min(len(non_fire_start_indices), 1000) \n",
    "    else:\n",
    "        num_non_fire_samples_to_use = int((num_fire_samples / fire_ratio) - num_fire_samples)\n",
    "        num_non_fire_samples_to_use = min(num_non_fire_samples_to_use, len(non_fire_start_indices))\n",
    "\n",
    "    fire_indices_to_use = fire_start_indices\n",
    "    # Ensure there are non-fire indices to choose from before sampling\n",
    "    if len(non_fire_start_indices) > 0 and num_non_fire_samples_to_use > 0:\n",
    "      non_fire_indices_to_use = np.random.choice(\n",
    "          non_fire_start_indices,\n",
    "          size=num_non_fire_samples_to_use,\n",
    "          replace=False \n",
    "      )\n",
    "      indices_to_use = np.concatenate([fire_indices_to_use, non_fire_indices_to_use])\n",
    "    else:\n",
    "      indices_to_use = np.array(fire_indices_to_use)\n",
    "\n",
    "    np.random.shuffle(indices_to_use)\n",
    "    indices_to_use = indices_to_use.astype(int)\n",
    "    \n",
    "    print(f\"Generator initialized. Found {len(fire_indices_to_use)} fire samples and using {len(indices_to_use) - len(fire_indices_to_use)} non-fire samples.\")\n",
    "\n",
    "    for i in indices_to_use:\n",
    "        seq_rows = df.iloc[i : i + SEQ_LEN]\n",
    "        horizon_rows = df.iloc[i + SEQ_LEN : i + SEQ_LEN + HORIZONS]\n",
    "        X, y = build_sample(seq_rows, horizon_rows, cache)\n",
    "        yield X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dbccc812-c0bf-4446-9bbe-8e352a0c2ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_dataset(df, cache, shuffle_buf=256):\n",
    "#     output_signature = (\n",
    "#         tf.TensorSpec(shape=(SEQ_LEN, PATCH_SIZE, PATCH_SIZE, 7), dtype=tf.float32),\n",
    "#         tf.TensorSpec(shape=(HORIZONS, PATCH_SIZE, PATCH_SIZE), dtype=tf.float32),\n",
    "#     )\n",
    "    \n",
    "#     ds = tf.data.Dataset.from_generator(\n",
    "#         lambda: make_generator(df, cache),\n",
    "#         output_signature=output_signature\n",
    "#     )\n",
    "    \n",
    "#     ds = ds.shuffle(shuffle_buf, reshuffle_each_iteration=True)\n",
    "#     ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "#     return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d67068e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(df, cache, shuffle=True, ensure_fire=True, shuffle_buf=256):\n",
    "    output_signature = (\n",
    "        tf.TensorSpec(shape=(SEQ_LEN, PATCH_SIZE, PATCH_SIZE, 7), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(HORIZONS, PATCH_SIZE, PATCH_SIZE), dtype=tf.float32),\n",
    "    )\n",
    "    \n",
    "    # CORRECTED LINE:\n",
    "    # Changed the keyword argument from 'ensure_fire=' to 'fire_ratio='\n",
    "    # This now correctly passes the value to the make_generator function.\n",
    "    ds = tf.data.Dataset.from_generator(\n",
    "        lambda: make_generator(df, cache, fire_ratio=ensure_fire),\n",
    "        output_signature=output_signature\n",
    "    )\n",
    "    \n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(shuffle_buf, reshuffle_each_iteration=True)\n",
    "    \n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e327f91e-e454-4b6c-8ea4-f6b8b6aae545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 17535\n",
      "Train samples: 14028\n",
      "Validation samples: 3507\n",
      "Loading rasters into memory...\n",
      "Loaded 9 rasters into memory ✅\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    csv_path = r\"C:\\Users\\Ankit\\Datasets_Forest_fire\\sequence_index_hourly_binary.csv\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    TOTAL = len(df)\n",
    "    VAL_SPLIT = 0.2\n",
    "    val_size = int(TOTAL * VAL_SPLIT)\n",
    "\n",
    "    val_df = df.iloc[:val_size].copy()\n",
    "    train_df = df.iloc[val_size:].copy()\n",
    "\n",
    "    print(f\"Total samples: {TOTAL}\")\n",
    "    print(f\"Train samples: {len(train_df)}\")\n",
    "    print(f\"Validation samples: {len(val_df)}\")\n",
    "\n",
    "    raster_cols = REQUIRED_COLS\n",
    "    print(\"Loading rasters into memory...\")\n",
    "    cache = load_rasters(df, raster_cols, max_workers=8)\n",
    "    print(f\"Loaded {len(cache)} rasters into memory ✅\")\n",
    "\n",
    "    # Use the new, balanced dataset functions\n",
    "    train_dataset = create_dataset(train_df, cache)\n",
    "    val_dataset = create_dataset(val_df, cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57d42604-5193-4582-b7a9-80bc3595e0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a467883b-f9ba-44cb-8ffe-1110e5ef2f89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90c9866-8427-4506-a99a-284d6c3d7964",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259f3ef4-6903-4f80-b057-8d0d44d4f238",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "90893721-b56d-4630-b86d-cf7a92ef7fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e700a567-9a37-440c-9e04-d691f0824fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 6           \n",
    "PATCH_H = 13            \n",
    "PATCH_W = 13          \n",
    "CHANNELS = 7       \n",
    "HORIZONS = 3            \n",
    "LSTM_UNITS = 64    \n",
    "CNN_FEATURES = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27354090-4f0c-46e7-a8d1-0d0897ea2f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers, models, callbacks\n",
    "\n",
    "# def build_conv_lstm_unet_model(\n",
    "#     seq_len=SEQ_LEN,\n",
    "#     patch_h=PATCH_H,\n",
    "#     patch_w=PATCH_W,\n",
    "#     channels=CHANNELS,\n",
    "#     horizons=HORIZONS\n",
    "# ):\n",
    "#     inp = layers.Input(shape=(seq_len, patch_h, patch_w, channels))\n",
    "\n",
    "#     enc1 = layers.ConvLSTM2D(\n",
    "#         filters=32, kernel_size=(3, 3), padding='same', return_sequences=True, activation='relu'\n",
    "#     )(inp)\n",
    "#     enc1_pool = layers.MaxPooling3D(pool_size=(1, 2, 2), padding='same')(enc1)\n",
    "\n",
    "#     enc2 = layers.ConvLSTM2D(\n",
    "#         filters=64, kernel_size=(3, 3), padding='same', return_sequences=True, activation='relu'\n",
    "#     )(enc1_pool)\n",
    "#     enc2_pool = layers.MaxPooling3D(pool_size=(1, 2, 2), padding='same')(enc2)\n",
    "\n",
    "#     bottleneck = layers.ConvLSTM2D(\n",
    "#         filters=128, kernel_size=(3, 3), padding='same', return_sequences=True, activation='relu'\n",
    "#     )(enc2_pool)\n",
    "\n",
    "#     dec1_up = layers.UpSampling3D(size=(1, 2, 2))(bottleneck)\n",
    "#     dec1_up = layers.Conv3D(filters=64, kernel_size=(3,3,3), padding='same', activation='relu')(dec1_up)\n",
    "#     dec1_up_cropped = layers.Cropping3D(cropping=((0, 0), (0, 1), (0, 1)))(dec1_up)\n",
    "#     dec1_concat = layers.Concatenate(axis=-1)([dec1_up_cropped, enc2])\n",
    "\n",
    "#     dec2_up = layers.UpSampling3D(size=(1, 2, 2))(dec1_concat)\n",
    "#     dec2_up = layers.Conv3D(filters=32, kernel_size=(3,3,3), padding='same', activation='relu')(dec2_up)\n",
    "#     dec2_up_cropped = layers.Cropping3D(cropping=((0, 0), (0, 1), (0, 1)))(dec2_up)\n",
    "#     dec2_concat = layers.Concatenate(axis=-1)([dec2_up_cropped, enc1])\n",
    "\n",
    "#     output_convlstm = layers.ConvLSTM2D(\n",
    "#         filters=1, kernel_size=(3, 3), padding='same', return_sequences=True, activation='sigmoid'\n",
    "#     )(dec2_concat[:, :horizons])\n",
    "\n",
    "#     final_output = tf.keras.ops.squeeze(output_convlstm, axis=-1)\n",
    "\n",
    "#     model = models.Model(inputs=inp, outputs=final_output)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "638c407e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "\n",
    "# --- Custom Function Definitions (The Fix for Lambda Layers) ---\n",
    "\n",
    "# Function 1: Slicing the time axis (for 'output_sliced')\n",
    "def slice_output_func(x):\n",
    "    # This slices the output sequence from length SEQ_LEN to HORIZONS (e.g., 6 to 3)\n",
    "    # The 'horizons' variable needs to be accessed via model scope or fixed value (3 in your case)\n",
    "    # Using HORIZONS global value for clarity, but the saved model config uses the value 3.\n",
    "    return x[:, :HORIZONS, :, :, :]\n",
    "\n",
    "# Function to calculate the output shape for slicing\n",
    "def slice_output_shape(input_shape):\n",
    "    # Input shape: (Batch, SEQ_LEN, H, W, 1) -> Output shape: (Batch, HORIZONS, H, W, 1)\n",
    "    # Only the time dimension (index 1) changes to HORIZONS (which is 3)\n",
    "    return (input_shape[0], HORIZONS, input_shape[2], input_shape[3], input_shape[4])\n",
    "\n",
    "# Function 2: Squeezing the channel axis (for 'final_output')\n",
    "def squeeze_output_func(x):\n",
    "    return tf.squeeze(x, axis=-1)\n",
    "\n",
    "# Function to calculate the output shape for squeezing\n",
    "def squeeze_output_shape(input_shape):\n",
    "    # Input shape: (Batch, T, H, W, 1) -> Output shape: (Batch, T, H, W)\n",
    "    # The last dimension (index 4) is removed\n",
    "    return input_shape[:-1] \n",
    "\n",
    "def build_conv_lstm_unet_model(\n",
    "    seq_len=SEQ_LEN,\n",
    "    patch_h=PATCH_H,\n",
    "    patch_w=PATCH_W,\n",
    "    channels=CHANNELS,\n",
    "    horizons=HORIZONS\n",
    "):\n",
    "    inp = layers.Input(shape=(seq_len, patch_h, patch_w, channels))\n",
    "    # ... (Encoder and Bottleneck layers remain the same) ...\n",
    "\n",
    "    # --- ENCODER 1 ---\n",
    "    enc1 = layers.ConvLSTM2D(filters=32, kernel_size=(3, 3), padding='same', return_sequences=True, activation='relu')(inp)\n",
    "    enc1_pool = layers.MaxPooling3D(pool_size=(1, 2, 2), padding='same')(enc1)\n",
    "\n",
    "    # --- ENCODER 2 ---\n",
    "    enc2 = layers.ConvLSTM2D(filters=64, kernel_size=(3, 3), padding='same', return_sequences=True, activation='relu')(enc1_pool)\n",
    "    enc2_pool = layers.MaxPooling3D(pool_size=(1, 2, 2), padding='same')(enc2)\n",
    "\n",
    "    # --- BOTTLENECK ---\n",
    "    bottleneck = layers.ConvLSTM2D(filters=128, kernel_size=(3, 3), padding='same', return_sequences=True, activation='relu')(enc2_pool)\n",
    "\n",
    "    # --- DECODER 1 ---\n",
    "    dec1_up = layers.UpSampling3D(size=(1, 2, 2))(bottleneck)\n",
    "    dec1_up = layers.Conv3D(filters=64, kernel_size=(3,3,3), padding='same', activation='relu')(dec1_up)\n",
    "    dec1_up_cropped = layers.Cropping3D(cropping=((0, 0), (0, 1), (0, 1)))(dec1_up)\n",
    "    dec1_concat = layers.Concatenate(axis=-1)([dec1_up_cropped, enc2])\n",
    "\n",
    "    # --- DECODER 2 ---\n",
    "    dec2_up = layers.UpSampling3D(size=(1, 2, 2))(dec1_concat)\n",
    "    dec2_up = layers.Conv3D(filters=32, kernel_size=(3,3,3), padding='same', activation='relu')(dec2_up)\n",
    "    dec2_up_cropped = layers.Cropping3D(cropping=((0, 0), (0, 1), (0, 1)))(dec2_up)\n",
    "    dec2_concat = layers.Concatenate(axis=-1)([dec2_up_cropped, enc1])\n",
    "\n",
    "    # --- FINAL LAYERS ---\n",
    "    output_convlstm = layers.ConvLSTM2D(\n",
    "        filters=1, kernel_size=(3, 3), padding='same', return_sequences=True, activation='sigmoid'\n",
    "    )(dec2_concat)\n",
    "\n",
    "    # FIX 1: Use named function with output_shape specified\n",
    "    output_sliced = layers.Lambda(\n",
    "        slice_output_func, \n",
    "        output_shape=slice_output_shape,\n",
    "        name='output_slicer'\n",
    "    )(output_convlstm)\n",
    "\n",
    "    # FIX 2: Use named function with output_shape specified\n",
    "    final_output = layers.Lambda(\n",
    "        squeeze_output_func,\n",
    "        output_shape=squeeze_output_shape,\n",
    "        name='final_squeeze'\n",
    "    )(output_sliced)\n",
    "    \n",
    "    model = models.Model(inputs=inp, outputs=final_output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e92766c1-8599-4ad5-8b12-7b77371b345b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bd2f64e9-74fb-4b7c-9ffc-de3555b0238c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)      │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv_lstm2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ConvLSTM2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">45,056</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ max_pooling3d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling3D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv_lstm2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv_lstm2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ConvLSTM2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">221,440</span> │ max_pooling3d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ max_pooling3d_1               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv_lstm2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling3D</span>)                │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv_lstm2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ConvLSTM2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">885,248</span> │ max_pooling3d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ up_sampling3d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling3D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv_lstm2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv3d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">221,248</span> │ up_sampling3d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ cropping3d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cropping3D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]               │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ concatenate (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ cropping3d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],          │\n",
       "│                               │                           │                 │ conv_lstm2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ up_sampling3d_1               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling3D</span>)                │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv3d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">110,624</span> │ up_sampling3d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ cropping3d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cropping3D</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ concatenate_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ cropping3d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│                               │                           │                 │ conv_lstm2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv_lstm2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ConvLSTM2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,344</span> │ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ output_slicer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv_lstm2d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ final_squeeze (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ output_slicer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m7\u001b[0m)      │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv_lstm2d (\u001b[38;5;33mConvLSTM2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │          \u001b[38;5;34m45,056\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ max_pooling3d (\u001b[38;5;33mMaxPooling3D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │               \u001b[38;5;34m0\u001b[0m │ conv_lstm2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv_lstm2d_1 (\u001b[38;5;33mConvLSTM2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │         \u001b[38;5;34m221,440\u001b[0m │ max_pooling3d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ max_pooling3d_1               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │               \u001b[38;5;34m0\u001b[0m │ conv_lstm2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "│ (\u001b[38;5;33mMaxPooling3D\u001b[0m)                │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv_lstm2d_2 (\u001b[38;5;33mConvLSTM2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │         \u001b[38;5;34m885,248\u001b[0m │ max_pooling3d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ up_sampling3d (\u001b[38;5;33mUpSampling3D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │               \u001b[38;5;34m0\u001b[0m │ conv_lstm2d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv3d (\u001b[38;5;33mConv3D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │         \u001b[38;5;34m221,248\u001b[0m │ up_sampling3d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ cropping3d (\u001b[38;5;33mCropping3D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │               \u001b[38;5;34m0\u001b[0m │ conv3d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]               │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ concatenate (\u001b[38;5;33mConcatenate\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │               \u001b[38;5;34m0\u001b[0m │ cropping3d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],          │\n",
       "│                               │                           │                 │ conv_lstm2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ up_sampling3d_1               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │               \u001b[38;5;34m0\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "│ (\u001b[38;5;33mUpSampling3D\u001b[0m)                │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv3d_1 (\u001b[38;5;33mConv3D\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m110,624\u001b[0m │ up_sampling3d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ cropping3d_1 (\u001b[38;5;33mCropping3D\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │               \u001b[38;5;34m0\u001b[0m │ conv3d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ concatenate_1 (\u001b[38;5;33mConcatenate\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │               \u001b[38;5;34m0\u001b[0m │ cropping3d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│                               │                           │                 │ conv_lstm2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv_lstm2d_3 (\u001b[38;5;33mConvLSTM2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │           \u001b[38;5;34m2,344\u001b[0m │ concatenate_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ output_slicer (\u001b[38;5;33mLambda\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │               \u001b[38;5;34m0\u001b[0m │ conv_lstm2d_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ final_squeeze (\u001b[38;5;33mLambda\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │ output_slicer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,485,960</span> (5.67 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,485,960\u001b[0m (5.67 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,485,960</span> (5.67 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,485,960\u001b[0m (5.67 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = build_conv_lstm_unet_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "040d3ca1-db4a-4d5d-ae17-2848bcbeaa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics=[tf.keras.metrics.BinaryAccuracy(), tf.keras.metrics.AUC()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f87cb8a8-b977-489f-a3cd-e8e4d93cad9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "checkpoint = callbacks.ModelCheckpoint(\n",
    "    \"best_unet_model.keras\", # Changed filename\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9d1ac3e8-da0e-47ab-b52a-b15d24e239e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 16\n",
      "Steps per epoch: 876\n",
      "Validation steps: 219\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 16\n",
    "steps_per_epoch = len(train_df) // BATCH_SIZE\n",
    "validation_steps = len(val_df) // BATCH_SIZE\n",
    "\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Steps per epoch: {steps_per_epoch}\")\n",
    "print(f\"Validation steps: {validation_steps}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463bfe97-c69b-4070-9648-e56b21827889",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "54aacb94-0770-44f8-bd4f-ea2f79b1cf3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "Scanning data for fire and non-fire events...\n",
      "Generator initialized. Found 14020 fire samples and using 0 non-fire samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1758646730.610202  409982 service.cc:152] XLA service 0x7f98440068e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1758646730.610219  409982 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 5060 Ti, Compute Capability 12.0\n",
      "2025-09-23 22:28:50.874322: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1758646731.834813  409982 cuda_dnn.cc:529] Loaded cuDNN version 91100\n",
      "I0000 00:00:1758646739.310925  409982 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m875/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - auc: 0.5325 - binary_accuracy: 0.9954 - loss: 0.0911Scanning data for fire and non-fire events...\n",
      "Generator initialized. Found 3499 fire samples and using 0 non-fire samples.\n",
      "\n",
      "Epoch 1: val_loss improved from None to 0.03133, saving model to best_unet_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 35ms/step - auc: 0.5720 - binary_accuracy: 0.9952 - loss: 0.0463 - val_auc: 0.6382 - val_binary_accuracy: 0.9948 - val_loss: 0.0313\n",
      "Epoch 2/50\n",
      "\u001b[1m  1/876\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:12:45\u001b[0m 5s/step - auc: 0.6670 - binary_accuracy: 0.9778 - loss: 0.1131Scanning data for fire and non-fire events...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-23 22:29:35.866010: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "2025-09-23 22:29:35.866024: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_4]]\n",
      "/usr/lib/python3.13/site-packages/keras/src/trainers/epoch_iterator.py:164: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator initialized. Found 3499 fire samples and using 0 non-fire samples.\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.03133\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - auc: 0.6670 - binary_accuracy: 0.9778 - loss: 0.1131 - val_auc: 0.6441 - val_binary_accuracy: 0.9948 - val_loss: 0.0315\n",
      "Epoch 3/50\n",
      "Scanning data for fire and non-fire events...\n",
      "Generator initialized. Found 14020 fire samples and using 0 non-fire samples.\n",
      "\u001b[1m874/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - auc: 0.6292 - binary_accuracy: 0.9950 - loss: 0.0304Scanning data for fire and non-fire events...\n",
      "Generator initialized. Found 3499 fire samples and using 0 non-fire samples.\n",
      "\n",
      "Epoch 3: val_loss improved from 0.03133 to 0.03108, saving model to best_unet_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 29ms/step - auc: 0.6344 - binary_accuracy: 0.9952 - loss: 0.0293 - val_auc: 0.6588 - val_binary_accuracy: 0.9948 - val_loss: 0.0311\n",
      "Epoch 4/50\n",
      "\u001b[1m  1/876\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - auc: 0.6925 - binary_accuracy: 0.9951 - loss: 0.0292Scanning data for fire and non-fire events...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-23 22:30:11.368007: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_4]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator initialized. Found 3499 fire samples and using 0 non-fire samples.\n",
      "\n",
      "Epoch 4: val_loss improved from 0.03108 to 0.03108, saving model to best_unet_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - auc: 0.6925 - binary_accuracy: 0.9951 - loss: 0.0292 - val_auc: 0.6583 - val_binary_accuracy: 0.9948 - val_loss: 0.0311\n",
      "Epoch 5/50\n",
      "Scanning data for fire and non-fire events...\n",
      "Generator initialized. Found 14020 fire samples and using 0 non-fire samples.\n",
      "\u001b[1m874/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - auc: 0.6372 - binary_accuracy: 0.9953 - loss: 0.0288Scanning data for fire and non-fire events...\n",
      "Generator initialized. Found 3499 fire samples and using 0 non-fire samples.\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.03108\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 29ms/step - auc: 0.6404 - binary_accuracy: 0.9952 - loss: 0.0292 - val_auc: 0.6435 - val_binary_accuracy: 0.9948 - val_loss: 0.0311\n",
      "Epoch 6/50\n",
      "\u001b[1m  1/876\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - auc: 0.7481 - binary_accuracy: 0.9995 - loss: 0.0070Scanning data for fire and non-fire events...\n",
      "Generator initialized. Found 3499 fire samples and using 0 non-fire samples.\n",
      "\n",
      "Epoch 6: val_loss improved from 0.03108 to 0.03105, saving model to best_unet_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - auc: 0.7481 - binary_accuracy: 0.9995 - loss: 0.0070 - val_auc: 0.6473 - val_binary_accuracy: 0.9948 - val_loss: 0.0310\n",
      "Epoch 7/50\n",
      "Scanning data for fire and non-fire events...\n",
      "Generator initialized. Found 14020 fire samples and using 0 non-fire samples.\n",
      "\u001b[1m875/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - auc: 0.6443 - binary_accuracy: 0.9952 - loss: 0.0291Scanning data for fire and non-fire events...\n",
      "Generator initialized. Found 3499 fire samples and using 0 non-fire samples.\n",
      "\n",
      "Epoch 7: val_loss improved from 0.03105 to 0.03104, saving model to best_unet_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 29ms/step - auc: 0.6407 - binary_accuracy: 0.9952 - loss: 0.0292 - val_auc: 0.6412 - val_binary_accuracy: 0.9948 - val_loss: 0.0310\n",
      "Epoch 8/50\n",
      "\u001b[1m  1/876\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - auc: 0.8134 - binary_accuracy: 0.9985 - loss: 0.0117Scanning data for fire and non-fire events...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-23 22:31:21.564035: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_4]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator initialized. Found 3499 fire samples and using 0 non-fire samples.\n",
      "\n",
      "Epoch 8: val_loss improved from 0.03104 to 0.03102, saving model to best_unet_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - auc: 0.8134 - binary_accuracy: 0.9985 - loss: 0.0117 - val_auc: 0.6416 - val_binary_accuracy: 0.9948 - val_loss: 0.0310\n",
      "Epoch 9/50\n",
      "Scanning data for fire and non-fire events...\n",
      "Generator initialized. Found 14020 fire samples and using 0 non-fire samples.\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - auc: 0.6381 - binary_accuracy: 0.9951 - loss: 0.0295Scanning data for fire and non-fire events...\n",
      "Generator initialized. Found 3499 fire samples and using 0 non-fire samples.\n",
      "\n",
      "Epoch 9: val_loss improved from 0.03102 to 0.03095, saving model to best_unet_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 29ms/step - auc: 0.6386 - binary_accuracy: 0.9952 - loss: 0.0292 - val_auc: 0.6609 - val_binary_accuracy: 0.9948 - val_loss: 0.0310\n",
      "Epoch 10/50\n",
      "\u001b[1m  1/876\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - auc: 0.6794 - binary_accuracy: 0.9990 - loss: 0.0100Scanning data for fire and non-fire events...\n",
      "Generator initialized. Found 3499 fire samples and using 0 non-fire samples.\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.03095\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - auc: 0.6794 - binary_accuracy: 0.9990 - loss: 0.0100 - val_auc: 0.6525 - val_binary_accuracy: 0.9948 - val_loss: 0.0310\n",
      "Epoch 11/50\n",
      "Scanning data for fire and non-fire events...\n",
      "Generator initialized. Found 14020 fire samples and using 0 non-fire samples.\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - auc: 0.6441 - binary_accuracy: 0.9955 - loss: 0.0276Scanning data for fire and non-fire events...\n",
      "Generator initialized. Found 3499 fire samples and using 0 non-fire samples.\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.03095\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 30ms/step - auc: 0.6505 - binary_accuracy: 0.9952 - loss: 0.0291 - val_auc: 0.6464 - val_binary_accuracy: 0.9948 - val_loss: 0.0310\n",
      "Epoch 12/50\n",
      "\u001b[1m  1/876\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - auc: 0.0000e+00 - binary_accuracy: 1.0000 - loss: 0.0045Scanning data for fire and non-fire events...\n",
      "Generator initialized. Found 3499 fire samples and using 0 non-fire samples.\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.03095\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - auc: 0.0000e+00 - binary_accuracy: 1.0000 - loss: 0.0045 - val_auc: 0.6444 - val_binary_accuracy: 0.9948 - val_loss: 0.0310\n",
      "Epoch 13/50\n",
      "Scanning data for fire and non-fire events...\n",
      "Generator initialized. Found 14020 fire samples and using 0 non-fire samples.\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - auc: 0.6550 - binary_accuracy: 0.9951 - loss: 0.0294Scanning data for fire and non-fire events...\n",
      "Generator initialized. Found 3499 fire samples and using 0 non-fire samples.\n",
      "\n",
      "Epoch 13: val_loss improved from 0.03095 to 0.03090, saving model to best_unet_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 30ms/step - auc: 0.6511 - binary_accuracy: 0.9952 - loss: 0.0291 - val_auc: 0.6732 - val_binary_accuracy: 0.9948 - val_loss: 0.0309\n",
      "Epoch 14/50\n",
      "\u001b[1m  1/876\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - auc: 0.6392 - binary_accuracy: 0.9990 - loss: 0.0104Scanning data for fire and non-fire events...\n",
      "Generator initialized. Found 3499 fire samples and using 0 non-fire samples.\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.03090\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - auc: 0.6392 - binary_accuracy: 0.9990 - loss: 0.0104 - val_auc: 0.6739 - val_binary_accuracy: 0.9948 - val_loss: 0.0309\n",
      "Epoch 15/50\n",
      "Scanning data for fire and non-fire events...\n",
      "Generator initialized. Found 14020 fire samples and using 0 non-fire samples.\n",
      "\u001b[1m875/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - auc: 0.6552 - binary_accuracy: 0.9953 - loss: 0.0284Scanning data for fire and non-fire events...\n",
      "Generator initialized. Found 3499 fire samples and using 0 non-fire samples.\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.03090\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 31ms/step - auc: 0.6529 - binary_accuracy: 0.9952 - loss: 0.0291 - val_auc: 0.6148 - val_binary_accuracy: 0.9948 - val_loss: 0.0313\n",
      "Epoch 16/50\n",
      "\u001b[1m  1/876\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - auc: 0.0000e+00 - binary_accuracy: 1.0000 - loss: 0.0038Scanning data for fire and non-fire events...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-23 22:33:44.002168: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_4]]\n",
      "2025-09-23 22:33:44.002192: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 2109665068740041596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator initialized. Found 3499 fire samples and using 0 non-fire samples.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.03090\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - auc: 0.0000e+00 - binary_accuracy: 1.0000 - loss: 0.0038 - val_auc: 0.6127 - val_binary_accuracy: 0.9948 - val_loss: 0.0314\n",
      "Epoch 17/50\n",
      "Scanning data for fire and non-fire events...\n",
      "Generator initialized. Found 14020 fire samples and using 0 non-fire samples.\n",
      "\u001b[1m874/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - auc: 0.6493 - binary_accuracy: 0.9952 - loss: 0.0291Scanning data for fire and non-fire events...\n",
      "Generator initialized. Found 3499 fire samples and using 0 non-fire samples.\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.03090\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 31ms/step - auc: 0.6548 - binary_accuracy: 0.9952 - loss: 0.0290 - val_auc: 0.6326 - val_binary_accuracy: 0.9948 - val_loss: 0.0311\n",
      "Epoch 18/50\n",
      "\u001b[1m  1/876\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - auc: 0.0000e+00 - binary_accuracy: 1.0000 - loss: 0.0040Scanning data for fire and non-fire events...\n",
      "Generator initialized. Found 3499 fire samples and using 0 non-fire samples.\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.03090\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - auc: 0.0000e+00 - binary_accuracy: 1.0000 - loss: 0.0040 - val_auc: 0.6272 - val_binary_accuracy: 0.9948 - val_loss: 0.0311\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=50,\n",
    "    callbacks=[early_stop, checkpoint],\n",
    "    verbose=1,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_steps=validation_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af9b5a64-5137-4d7b-a519-c08f8ad5639f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load model from: C:\\Users\\Ankit\\Downloads\\best_unet_model (1).h5\n",
      "\n",
      "[ERROR] Model loading failed. You may need to try 'lambda_2' for the second function's key. Error: Exception encountered when calling Lambda.call().\n",
      "\n",
      "\u001b[1mWe could not automatically infer the shape of the Lambda's output. Please specify the `output_shape` argument for this Lambda layer.\u001b[0m\n",
      "\n",
      "Arguments received by Lambda.call():\n",
      "  • args=('<KerasTensor shape=(None, 3, 13, 13, 1), dtype=float32, sparse=False, ragged=False, name=keras_tensor_321>',)\n",
      "  • kwargs={'mask': 'None'}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import keras # Use keras explicitly for backend functions\n",
    "\n",
    "# --- 1. Re-Define the Lambda Functions ---\n",
    "\n",
    "# Function 1: Slicing the time axis (from 6 to 3)\n",
    "# Original code: lambda x: x[:, :horizons, :, :, :]\n",
    "def slice_output(x):\n",
    "    # Horizons is 3, Sequence length is 6. This slices the first 3 time steps.\n",
    "    # The 'horizons' variable is not available globally during load, so we use its value (3).\n",
    "    # Keras will look for the exact name 'slice_output' or similar if it were named in the model config.\n",
    "    return x[:, :3, :, :, :]\n",
    "\n",
    "# Function 2: Squeezing the channel axis (removing the last dimension: 1)\n",
    "# Original code: lambda x: tf.squeeze(x, axis=-1)\n",
    "def squeeze_output(x):\n",
    "    return tf.squeeze(x, axis=-1)\n",
    "\n",
    "# --- 2. Define the Output Shape Functions ---\n",
    "\n",
    "# Keras requires a function to calculate the output shape for complex operations.\n",
    "\n",
    "def slice_output_shape(input_shape):\n",
    "    # Input shape: (None, 6, 13, 13, 1) -> Output shape: (3, 13, 13, 1)\n",
    "    # The batch dim (0) and time dim (1) change (or are restricted).\n",
    "    # Since horizons=3, the time dimension (index 1) becomes 3.\n",
    "    return (3, input_shape[2], input_shape[3], input_shape[4])\n",
    "\n",
    "def squeeze_output_shape(input_shape):\n",
    "    # Input shape: (None, 3, 13, 13, 1) -> Output shape: (3, 13, 13)\n",
    "    # The last dimension (index 4) is removed.\n",
    "    return input_shape[:-1] \n",
    "\n",
    "# --- 3. Create the Custom Objects Dictionary ---\n",
    "# The keys used below ('lambda' and 'lambda_1') are the default names \n",
    "# Keras gives to unnamed lambda layers when saving to .h5. \n",
    "# We must register the functions with their corresponding shape calculators.\n",
    "CUSTOM_OBJECTS = {\n",
    "    # 💡 Note: If you used a custom loss like 'dice_loss', you must add it here too!\n",
    "    # 'dice_loss': dice_loss, \n",
    "    \n",
    "    # Register the Slicing Lambda layer\n",
    "    # Keras typically names the first one 'lambda' or 'lambda_1'\n",
    "    'lambda': tf.keras.layers.Lambda(\n",
    "        slice_output, \n",
    "        output_shape=slice_output_shape\n",
    "    ),\n",
    "    \n",
    "    # Register the Squeeze Lambda layer (This is the one causing the error in your trace)\n",
    "    # Keras typically names the second one 'lambda_1' or 'lambda_2'\n",
    "    'lambda_1': tf.keras.layers.Lambda(\n",
    "        squeeze_output, \n",
    "        output_shape=squeeze_output_shape\n",
    "    )\n",
    "} \n",
    "\n",
    "# --- 4. Load the Model Safely ---\n",
    "# Your model was saved as \"best_unet_model (1).h5\"\n",
    "MODEL_PATH = r\"C:\\Users\\Ankit\\Downloads\\best_unet_model (1).h5\"\n",
    "\n",
    "try:\n",
    "    print(f\"Attempting to load model from: {MODEL_PATH}\")\n",
    "\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        raise FileNotFoundError(f\"Model file not found at: {MODEL_PATH}\")\n",
    "\n",
    "    # Use safe_mode=False (to allow lambda functions) \n",
    "    # and custom_objects (to provide shape metadata)\n",
    "    model = tf.keras.models.load_model(\n",
    "        MODEL_PATH, \n",
    "        custom_objects=CUSTOM_OBJECTS, \n",
    "        safe_mode=False  \n",
    "    )\n",
    "\n",
    "    print(\"\\nModel loaded successfully!\")\n",
    "    model.summary() \n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n[ERROR] Model loading failed. You may need to try 'lambda_2' for the second function's key. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba29477-0f3a-4c90-8253-a9ccc9626ea3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa53c55-214c-407a-a710-1fc22c9e73e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading model from best_unet_model.h5: Requested the deserialization of a `Lambda` layer whose `function` is a Python lambda. This carries a potential risk of arbitrary code execution and thus it is disallowed by default. If you trust the source of the artifact, you can override this error by passing `safe_mode=False` to the loading function, or calling `keras.config.enable_unsafe_deserialization().\n",
      "\n",
      "Evaluating model performance on the validation set...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'loaded_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# --- 3. Evaluate the Model on the Validation Dataset ---\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEvaluating model performance on the validation set...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m evaluation_results = \u001b[43mloaded_model\u001b[49m.evaluate(val_dataset_for_evaluation)\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# The 'evaluate' method returns the loss followed by the metrics in the order you compiled them.\u001b[39;00m\n\u001b[32m     37\u001b[39m loss = evaluation_results[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'loaded_model' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Assuming your 'val_df' and 'cache' from your data preparation script are in memory.\n",
    "\n",
    "# --- 1. Load the Model ---\n",
    "# Ensure the path to your saved model is correct.\n",
    "model_path = r\"best_unet_model.h5\"\n",
    "try:\n",
    "    loaded_model = tf.keras.models.load_model(model_path)\n",
    "    print(\"Model loaded successfully!\")\n",
    "    loaded_model.summary()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model from {model_path}: {e}\")\n",
    "    # Exit the script if the model cannot be loaded\n",
    "    exit()\n",
    "\n",
    "# --- 2. Create the Validation Dataset for Evaluation ---\n",
    "# Use your existing 'create_dataset' function.\n",
    "# Do NOT shuffle the dataset for evaluation.\n",
    "# Do NOT force fire samples as we want to test on the natural distribution.\n",
    "val_dataset_for_evaluation = create_dataset(\n",
    "    val_df,\n",
    "    cache,\n",
    "    shuffle=False,\n",
    "    ensure_fire=False\n",
    ")\n",
    "\n",
    "# Batch the dataset using the same batch size as training.\n",
    "BATCH_SIZE = 2 # Change this to your training batch size if different\n",
    "val_dataset_for_evaluation = val_dataset_for_evaluation.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# --- 3. Evaluate the Model on the Validation Dataset ---\n",
    "print(\"\\nEvaluating model performance on the validation set...\")\n",
    "evaluation_results = loaded_model.evaluate(val_dataset_for_evaluation)\n",
    "\n",
    "# The 'evaluate' method returns the loss followed by the metrics in the order you compiled them.\n",
    "loss = evaluation_results[0]\n",
    "binary_accuracy = evaluation_results[1]\n",
    "auc = evaluation_results[2]\n",
    "\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Binary Accuracy: {binary_accuracy:.4f}\")\n",
    "print(f\"Test AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7128902-c4fc-4945-af10-c7422f13b3bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcb5b8d6-7d3e-445d-a22a-6122754be6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TensorFlow and System Information ---\n",
      "TensorFlow Version: 2.20.0\n",
      "Python Version: 3.12.11 (main, Jun  6 2025, 22:10:26) [GCC 15.1.1 20250425]\n",
      "\n",
      "--- GPU Detection ---\n",
      "✅ Success! TensorFlow has detected 1 GPU(s).\n",
      "  GPU [0]:\n",
      "    Name: NVIDIA GeForce RTX 5060 Ti\n",
      "    Compute Capability: (12, 0)\n",
      "\n",
      "--- Simple GPU Operation Test ---\n",
      "✅ A simple matrix multiplication was successfully executed on the GPU.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import sys\n",
    "\n",
    "def verify_gpu():\n",
    "    \"\"\"\n",
    "    Checks if TensorFlow can detect and use the GPU.\n",
    "    \"\"\"\n",
    "    print(f\"--- TensorFlow and System Information ---\")\n",
    "    print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "    print(f\"Python Version: {sys.version}\")\n",
    "    \n",
    "    # The key function to check for GPUs\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    \n",
    "    print(f\"\\n--- GPU Detection ---\")\n",
    "    if gpus:\n",
    "        print(f\"✅ Success! TensorFlow has detected {len(gpus)} GPU(s).\")\n",
    "        try:\n",
    "            # Print details for each detected GPU\n",
    "            for i, gpu in enumerate(gpus):\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "                details = tf.config.experimental.get_device_details(gpu)\n",
    "                print(f\"  GPU [{i}]:\")\n",
    "                print(f\"    Name: {details.get('device_name', 'N/A')}\")\n",
    "                print(f\"    Compute Capability: {details.get('compute_capability', 'N/A')}\")\n",
    "        except RuntimeError as e:\n",
    "            print(f\"  ⚠️ Could not get GPU details: {e}\")\n",
    "            \n",
    "        print(\"\\n--- Simple GPU Operation Test ---\")\n",
    "        try:\n",
    "            # Perform a simple operation on the GPU\n",
    "            with tf.device('/GPU:0'):\n",
    "                a = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
    "                b = tf.constant([[1.0, 1.0], [0.0, 1.0]])\n",
    "                c = tf.matmul(a, b)\n",
    "            print(\"✅ A simple matrix multiplication was successfully executed on the GPU.\")\n",
    "        except RuntimeError as e:\n",
    "            print(f\"❌ Failed to execute a simple operation on the GPU: {e}\")\n",
    "\n",
    "    else:\n",
    "        print(\"❌ Failure. TensorFlow did NOT detect any GPUs.\")\n",
    "        print(\"Please check the following:\")\n",
    "        print(\"1. Is a compatible NVIDIA GPU installed?\")\n",
    "        print(\"2. Are the NVIDIA drivers installed correctly? (Check with 'nvidia-smi')\")\n",
    "        print(\"3. Are the CUDA and cuDNN versions compatible with your TensorFlow version?\")\n",
    "        print(\"   (See https://www.tensorflow.org/install/source#gpu)\")\n",
    "        print(\"4. Were CUDA and cuDNN installed correctly and are their paths accessible?\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    verify_gpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd6cfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd631643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- PyTorch and System Information ---\n",
      "PyTorch Version: 2.8.0\n",
      "Python Version: 3.13.7 (main, Aug 15 2025, 12:34:02) [GCC 15.2.1 20250813]\n",
      "\n",
      "--- GPU Detection ---\n",
      "✅ Success! PyTorch has detected 1 CUDA-enabled GPU(s).\n",
      "  GPU [0]: NVIDIA GeForce RTX 5060 Ti (Active)\n",
      "\n",
      "--- Simple GPU Operation Test ---\n",
      "  Attempting to use device: cuda:0 (NVIDIA GeForce RTX 5060 Ti)\n",
      "  Successfully created a tensor on the GPU.\n",
      "  Tensor: tensor([1.5000, 2.5000, 3.5000], device='cuda:0')\n",
      "  Tensor's Device: cuda:0\n",
      "\n",
      "  Performing a simple operation (tensor * 2)...\n",
      "  Result: tensor([3., 5., 7.], device='cuda:0')\n",
      "  Result's Device: cuda:0\n",
      "\n",
      "✅ GPU is working correctly for computations.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "\n",
    "def check_pytorch_gpu():\n",
    "    \"\"\"\n",
    "    Checks for GPU availability in PyTorch, prints details, and runs a test.\n",
    "    \"\"\"\n",
    "    print(f\"--- PyTorch and System Information ---\")\n",
    "    print(f\"PyTorch Version: {torch.__version__}\")\n",
    "    print(f\"Python Version: {sys.version}\")\n",
    "\n",
    "    # The primary function to check for a CUDA-enabled GPU\n",
    "    is_available = torch.cuda.is_available()\n",
    "\n",
    "    print(f\"\\n--- GPU Detection ---\")\n",
    "    if is_available:\n",
    "        # Get the number of available GPUs\n",
    "        gpu_count = torch.cuda.device_count()\n",
    "        print(f\"✅ Success! PyTorch has detected {gpu_count} CUDA-enabled GPU(s).\")\n",
    "\n",
    "        # Print details for each GPU\n",
    "        for i in range(gpu_count):\n",
    "            gpu_name = torch.cuda.get_device_name(i)\n",
    "            current_device_index = torch.cuda.current_device()\n",
    "            # Add a star '*' to indicate the currently active GPU\n",
    "            active_indicator = \" (Active)\" if i == current_device_index else \"\"\n",
    "            print(f\"  GPU [{i}]: {gpu_name}{active_indicator}\")\n",
    "\n",
    "        print(\"\\n--- Simple GPU Operation Test ---\")\n",
    "        try: # 1. Define the device to be the first available GPU\n",
    "            device = torch.device(\"cuda:0\")\n",
    "            print(f\"  Attempting to use device: {device} ({torch.cuda.get_device_name(0)})\")\n",
    "\n",
    "            # 2. Create a sample tensor and move it to the GPU\n",
    "            sample_tensor = torch.tensor([1.5, 2.5, 3.5], device=device)\n",
    "            print(f\"  Successfully created a tensor on the GPU.\")\n",
    "            print(f\"  Tensor: {sample_tensor}\")\n",
    "            print(f\"  Tensor's Device: {sample_tensor.device}\")\n",
    "\n",
    "            # 3. Perform a simple operation\n",
    "            result = sample_tensor * 2\n",
    "            print(\"\\n  Performing a simple operation (tensor * 2)...\")\n",
    "            print(f\"  Result: {result}\")\n",
    "            print(f\"  Result's Device: {result.device}\")\n",
    "            print(\"\\n✅ GPU is working correctly for computations.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ An error occurred during the GPU operation test: {e}\")\n",
    "\n",
    "    else:\n",
    "        print(\"❌ Failure. PyTorch did NOT detect any CUDA-enabled GPUs.\")\n",
    "        print(\"Please check the following:\")\n",
    "        print(\"1. Is a compatible NVIDIA GPU installed?\")\n",
    "        print(\"2. Are the NVIDIA drivers installed correctly? (Check with 'nvidia-smi' in your terminal)\")\n",
    "        print(\"3. Did you install the PyTorch version with CUDA support?\")\n",
    "        print(\"   (e.g., from pytorch.org, select a CUDA option in the install matrix)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    check_pytorch_gpu()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
